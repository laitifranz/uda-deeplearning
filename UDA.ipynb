{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "X8p69FxcivtS"
      },
      "source": [
        "# **Deep Learning Assignment 2022**\n",
        "\n",
        "## Unsupervised Domain Adaptation\n",
        "\n",
        "Student team:\n",
        "*   Laiti Francesco\n",
        "*   Lobba Davide\n",
        "\n",
        "---\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/laitifranz/uda-deeplearning/blob/main/UDA.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrwzWQa7HWsb"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2yZmLhfHWsc"
      },
      "source": [
        "In this notebook we build, train and evaluate two different deep learning frameworks with respect to a baseline, that involves the topic of Unsupervised Domain Adaptation (UDA). For this assignment we use a UDA benchmark constisting of two domains, Product $P$ and Real World $RW$, treated as source domain and target domain, and viceversa. The aim of this project is to \"propose a UDA technique to counteract the negative impact of the domain gap when training the model on the source distribution and evaluating it on the target distribution\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHY5HG-vHWsc"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSg-hpRSHWsd"
      },
      "source": [
        "We use the [Adaptiope](https://drive.google.com/file/d/1FmdsvetC0oVyrFJ9ER7fcN-cXPOWx2gq/view) object recognition dataset composed of 3 distinct domain: syntethic, product, real world. The original dataset has 123 object categories for each domain, but for this assignment we will use only 20 categories randomly chosen. We use the standard 80%/20% train/test split, as suggested in the assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShrnXoIVHWsd"
      },
      "source": [
        "### Method choices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swBcHuCzHWse"
      },
      "source": [
        "We decided to implement two different approaches to investigate the results and the performances on the Adaptiope dataset with respect to a baseline.\n",
        "\n",
        "For the delivery we implemented:\n",
        "- A baseline trained on the source domain and evaluated on the target domain **without** any domain alignment strategy involved, in order to obtain upper and lower bound for the accuracy;\n",
        "- The method proposed by [Maximum Classifier Discrepancy for Unsupervised Domain Adaptation (MCD_DA)](https://arxiv.org/abs/1712.02560) released in 2018 by Saito et. al.;\n",
        "- The method proposed by [Deep Subdomain Adaptation Network for Image Classification (DSAN)](https://arxiv.org/abs/2106.09388) released in 2021 by Zhu et. al.\n",
        "\n",
        "For all these experiments we use the ResNet18 as backbone model for computational restrictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKDblqI7HWsf"
      },
      "source": [
        "### Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6ZnSQgbHWsf"
      },
      "source": [
        "For each approach we trained and tested the methods in both directions, one at a time.\n",
        "- Regarding the baseline, we trained on $P$ training set and test on $RW$ test set. We compute the accuracy in order to obtain the lower bound accuracy for the domain adaptation task. Vice-versa for $RW$ to $P$. We also trained the baseline on each domain and test on the same domain test set in order to get the upper bound accuracy;\n",
        "- Regarding the second method, MCD_DA, we proceeded in a similar way but without computing the upper buond accuracy;\n",
        "- For the last method, DSAN, we conducted the same experiments of MCD_DA.\n",
        "\n",
        "In the notebook we declared a section for the global constants, valid for every method. Then every approach has its own constants section that matches with the settings (epochs, learning rate, optimizer...) of the method takes in consideration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0FPf624HWsg"
      },
      "source": [
        "### Requirements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOKZzE5HHWsg"
      },
      "source": [
        "To load the dataset, we use the built-in function of `google.colab` library to link the personal Drive to the virtual machine of GColab and `rsync` tool to get the status bar of the clone process. We expect the user to fix the dataset path according to the saved location of its own Drive. The file must be named `Adaptiope.zip`, that is the default name of the compressed folder.\n",
        "\n",
        "We added a path to save the best model weights for each approach.\n",
        "\n",
        "> Note that the entire workspace folder of this project is available [here](https://drive.google.com/drive/folders/1yyg4pHmEk3Jyc3T9xVX8M6z5nHpdpnhA?usp=sharing). You can simply create an alias or create a copy of the folder in your GDrive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAcNz0_jHWsh"
      },
      "source": [
        "## Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-yCd1CeHWsh"
      },
      "source": [
        "In this section we define the requirements to run correctly the notebook and load properly the dataset Adaptiope.\n",
        "\n",
        "To keep track of model performances and create charts regarding loss and accuracy, we use [WandB](https://wandb.ai/dlfl/projects)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHUgqENmHWsi"
      },
      "outputs": [],
      "source": [
        "!pip3 install wandb -qU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmpsFnZvHWsj"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTQScuYdHWsj"
      },
      "source": [
        "We import the necessary Python libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nV05a4PqgflT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "import yaml\n",
        "import copy\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3rfVHmiHWsk"
      },
      "source": [
        "### Declare global constants"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9u1QVk3HWsk"
      },
      "source": [
        "We declare the global constants used in this notebook valid for all the methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3jzyjoBHWsl"
      },
      "outputs": [],
      "source": [
        "# dataset settings\n",
        "NUM_CLASSES = 20 # subset of classes requested by the assignment\n",
        "CLASSES = [\"backpack\", \"bookcase\", \"car jack\", \"comb\", \"crown\", \"file cabinet\", \"flat iron\", \"game controller\", \"glasses\",\n",
        "           \"helicopter\", \"ice skates\", \"letter tray\", \"monitor\", \"mug\", \"network switch\", \"over-ear headphones\", \"pen\",\n",
        "           \"purse\", \"stand mixer\", \"stroller\"]\n",
        "BATCH_SIZE = 256\n",
        "SPLIT_RATIO = 0.8 # 80/20, as requested by the assignment\n",
        "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
        "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
        "\n",
        "# resnet18 parameters\n",
        "WEIGHTS_RESNET18 = \"IMAGENET1K_V1\"  # reference: https://pytorch.org/vision/stable/models.html \n",
        "\n",
        "# env\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# wandb setup\n",
        "WANDB_MODE = \"disabled\" # \"online\" to enable WandB\n",
        "PROJECT_NAME = \"DL_UDA_2022\"\n",
        "ENTITY = \"dlfl\"\n",
        "\n",
        "# paths\n",
        "DIRECTORY_P = \"/content/adaptiope_small/product_images\"\n",
        "DIRECTORY_RW = \"/content/adaptiope_small/real_life\"\n",
        "WEIGHTS_PATH = \"/content/weights/\"\n",
        "ASSETS_PATH = \"/content/visualizer/\"\n",
        "\n",
        "# for reproducibility\n",
        "g = torch.Generator()\n",
        "g.manual_seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9Sxvz-GHWsl"
      },
      "source": [
        "### Mount the drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFD3w9C-HWsl"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bFdagK4HWsm"
      },
      "source": [
        "### Prepare the Adaptiope dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxOBVQXlHWsm"
      },
      "source": [
        "We now download the dataset on the virtual machine and then create a subset of the dataset as requested by the assignment. Code adapted from the one provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCLS6SE_srJT"
      },
      "outputs": [],
      "source": [
        "!mkdir dataset\n",
        "!rsync --info=progress \"gdrive/My Drive/DL_UDA_2022/dataset/Adaptiope.zip\" dataset/\n",
        "!ls dataset\n",
        "!unzip -qq dataset/Adaptiope.zip\n",
        "!mkdir adaptiope_small\n",
        "\n",
        "for d, td in zip([\"Adaptiope/product_images\", \"Adaptiope/real_life\"], [\"adaptiope_small/product_images\", \"adaptiope_small/real_life\"]):\n",
        "  os.makedirs(td)\n",
        "  for c in tqdm(CLASSES):\n",
        "    c_path = os.path.join(d, c)\n",
        "    c_target = os.path.join(td, c)\n",
        "    shutil.copytree(c_path, c_target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZg7mLbsTtkh"
      },
      "source": [
        "### Utility functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myODsR-vqONt"
      },
      "source": [
        "We experienced an issue with the GPU memory where, after running different runs, CUDA interrupts the execution of cells with an error of full memory capacity reached. To partially address it, we collect most of the variables related to the GPU and free the cache.\n",
        "\n",
        "After some runs, we, unfortunately, need to restart the notebook kernel to use it again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zA6N9t6sT2d0"
      },
      "outputs": [],
      "source": [
        "def free_GPU_memory(*args):\n",
        "    for arg in args:\n",
        "      del arg\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKA47poA3PlU"
      },
      "source": [
        "## Dataset & Dataloaders\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joo42pa7HWsn"
      },
      "source": [
        "We create four dataloaders, two for each dataset, returned as dictionary for an easy access and better order:\n",
        "- Product $P$ train & test;\n",
        "- Real World $RW$ train & test.\n",
        "\n",
        "We also apply transformations to images in order to apply data augmentation and fit the image with the input sizes required by ResNet18. We also applied normalization metrics according to ImageNet mean and standard deviation.\n",
        "\n",
        "The split ratio is set to 80/20. We decided to drop the last uncomplete batch because could affect negatively the performance of the model and also to avoid bias issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOZj5-slB8SF"
      },
      "outputs": [],
      "source": [
        "# structure adapted from the labs\n",
        "def get_data(batch_size, img_root_product, img_root_realworld):\n",
        "\n",
        "  transform = list()\n",
        "  transform.append(T.Resize((256, 256)))                      # resize each PIL image to 256 x 256\n",
        "  transform.append(T.RandomCrop((224, 224)))                  # randomly crop a 224 x 224 patch\n",
        "  transform.append(T.ToTensor())                              # convert Numpy to Pytorch Tensor\n",
        "  transform.append(T.Normalize(mean=IMAGENET_MEAN, \n",
        "                               std=IMAGENET_STD))             # normalize with ImageNet mean & std\n",
        "  transform = T.Compose(transform)                            # compose the above transformations into one\n",
        "    \n",
        "  # load data\n",
        "  product_images_dataset = torchvision.datasets.ImageFolder(root=img_root_product, transform=transform)\n",
        "  real_images_dataset = torchvision.datasets.ImageFolder(root=img_root_realworld, transform=transform)\n",
        "\n",
        "  # create train and test splits\n",
        "  num_samples_product = len(product_images_dataset)\n",
        "  training_samples_product = int(num_samples_product * SPLIT_RATIO)\n",
        "  test_samples_product = num_samples_product - training_samples_product\n",
        "  \n",
        "  num_samples_real = len(real_images_dataset)\n",
        "  training_samples_real = int(num_samples_real * SPLIT_RATIO)\n",
        "  test_samples_real = num_samples_real - training_samples_real\n",
        "\n",
        "  training_data_product, test_data_product = torch.utils.data.random_split(product_images_dataset, [training_samples_product, test_samples_product], generator=g)\n",
        "  training_data_real, test_data_real = torch.utils.data.random_split(real_images_dataset, [training_samples_real, test_samples_real], generator=g)\n",
        "\n",
        "  # initialize dataloaders\n",
        "  product_train_loader = torch.utils.data.DataLoader(training_data_product, batch_size, shuffle=True, drop_last=True, generator=g) # we decided to drop the last incomplete batch\n",
        "  product_test_loader = torch.utils.data.DataLoader(test_data_product, batch_size, shuffle=False, generator=g)\n",
        "\n",
        "  realword_train_loader = torch.utils.data.DataLoader(training_data_real, batch_size, shuffle=True, drop_last=True, generator=g)\n",
        "  realworld_test_loader = torch.utils.data.DataLoader(test_data_real, batch_size, shuffle=False, generator=g)\n",
        "  \n",
        "  prod_dictionary = {'name': 'product',\n",
        "                     'train': product_train_loader,\n",
        "                     'test': product_test_loader}\n",
        "  rw_dictionary   = {'name': 'realworld',\n",
        "                     'train': realword_train_loader,\n",
        "                     'test': realworld_test_loader}\n",
        "  \n",
        "  return prod_dictionary, rw_dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKe5cSbFHWso"
      },
      "source": [
        "We create 2 dictionaries, one for Product and one for Real World. Each dictionary has three keys: \n",
        "- ``name`` store the name of the domain;\n",
        "- ``train`` store the train set dataloader of the domain; \n",
        "- ``test`` store the test set dataloader of the domain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eylZ-jm4HWso"
      },
      "outputs": [],
      "source": [
        "product_data, rw_data = get_data(batch_size=BATCH_SIZE, img_root_product=DIRECTORY_P, img_root_realworld=DIRECTORY_RW)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "356krQg-HWso"
      },
      "source": [
        "## Loss & Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0j0PRj7-HWsp"
      },
      "source": [
        "In this section we define the losses and optimizers used in this project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iezc8CYDHWsp"
      },
      "source": [
        "### Loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6fjLHMhHWsp"
      },
      "source": [
        "We declare one main loss, the Cross entropy loss $\\mathcal{L_{ce}}$ for supervised tasks. The objective is as follows:\n",
        "$$\\begin{aligned} \n",
        "\\mathcal{L_{ce}} = -\\sum_{c=1}^My_{o,c}\\log(p_{o,c})\n",
        "\\end{aligned}\n",
        "$$\n",
        "where $M$ is number of classes, $y$ is the binary indicator (0 or 1) if class label $c$ is the correct classification for observation $o$, and $p$ is the predicted probability observation $o$ of class $c$.\n",
        " \n",
        "The other losses for UDA tasks are described and declared when the methods are presented."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0iYPC2-zaD8"
      },
      "outputs": [],
      "source": [
        "def get_cost_function():\n",
        "  return torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDEIVnxTHWsp"
      },
      "source": [
        "### Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3IfwDevHWsp"
      },
      "source": [
        "We declare two functions:\n",
        "- An optimizer, choose through the ``selection`` parameter that apply the learning rate to the model parameters :\n",
        "  - SGD: return a Stochastic Gradient Descent optimizer;\n",
        "  - Adam: return an Adam optimizer.\n",
        "- A scheduler. The papers do not introduced a scheduler, but to potentially achieve better performances we adopted the exponential scheduler with a defined gamma."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6shviOEBHWsq"
      },
      "outputs": [],
      "source": [
        "def get_optimizer(parameters, selection, lr, weight_decay=0, momentum=0):\n",
        "    if selection == 'SGD':\n",
        "        optimizer = torch.optim.SGD(parameters, lr=lr, weight_decay=weight_decay, momentum=momentum)\n",
        "    elif selection == 'Adam':\n",
        "        optimizer = torch.optim.Adam(parameters, lr=lr, weight_decay=weight_decay)\n",
        "    else:\n",
        "        raise NameError(f\"Optimizer {selection} not recognized\")\n",
        "    return optimizer\n",
        "\n",
        "def get_scheduler(optimizer, gamma):\n",
        "    return torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXbvUywLHWsq"
      },
      "source": [
        "## Performance visualizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6QW-AmlHWsq"
      },
      "source": [
        "In this section we declare first a method to get the predictions, then we define three functions for results visualization:\n",
        "- T-SNE algorithm to plot the visualization samples in a 2D space;\n",
        "- Classification report;\n",
        "- Confusion matrix.\n",
        "\n",
        "Then we define a function to call all the three methods to performe the visualization of the results.\n",
        "\n",
        "> Note that all the results will be saved in a local directory of the VM. We uploaded them on Google Drive to link them in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGk8XmIxHWsq"
      },
      "outputs": [],
      "source": [
        "def get_predictions(model, dataloader):\n",
        "    y_features_list = []\n",
        "    y_true_list = []\n",
        "    y_pred_list = []\n",
        "\n",
        "    if type(model) is dict: # manage the 3-model case of MCD_DA\n",
        "        model[\"gen\"].eval()\n",
        "        model[\"clf1\"].eval()\n",
        "        model[\"clf2\"].eval()\n",
        "    else:\n",
        "        model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, labels) in enumerate(dataloader):\n",
        "            inputs = inputs.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "            \n",
        "            if type(model) is dict:\n",
        "                feat     = model[\"gen\"](inputs)\n",
        "                features = model[\"clf1\"].forward_features(feat)\n",
        "                outputs  = model[\"clf1\"](feat) # we choose the classifier 1 to display the results, as the relative paper did\n",
        "            else:\n",
        "                features = model.forward_features(inputs) # get features for t-SNE visualization\n",
        "                outputs = model(inputs) # get probability from the linear classifier\n",
        "            preds = outputs.argmax(dim=1)\n",
        "\n",
        "            y_features_list.append(features)\n",
        "            y_true_list.append(labels)\n",
        "            y_pred_list.append(preds)\n",
        "\n",
        "    y_features = torch.cat(y_features_list).cpu().numpy()\n",
        "    y_true = torch.cat(y_true_list).cpu().numpy()\n",
        "    y_pred = torch.cat(y_pred_list).cpu().numpy()\n",
        "\n",
        "    free_GPU_memory(inputs, labels, y_features_list, y_true_list, y_pred_list)\n",
        "    \n",
        "    return y_features, y_true, y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsDk8T1THWsr"
      },
      "source": [
        "#### T-SNE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCxn6MtsuFkr"
      },
      "source": [
        "We define two functions:\n",
        "- ``get_tsne`` generates the t-SNE regarding the classes of target samples;\n",
        "- ``get_tsne_src_tgt`` generates the t-SNE regarding the source and target samples.\n",
        "\n",
        "Both functions take in input the features extracted from the last layer of the network, before the final linear classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBrh_PnxHWsr"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "from matplotlib.pyplot import cm\n",
        "\n",
        "def get_tsne(labels, features, method_name): \n",
        "    tsne = TSNE(perplexity=20, n_components=2, init=\"pca\", learning_rate='auto', random_state=42).fit_transform(features)\n",
        "\n",
        "    category_to_color = {}\n",
        "    color = cm.rainbow(np.linspace(0, 1, NUM_CLASSES))\n",
        "    for i, c in zip(range(0,NUM_CLASSES), color):\n",
        "        category_to_color[i] = c\n",
        "\n",
        "    category_to_label = {}\n",
        "    for i, c in zip(range(0,NUM_CLASSES), CLASSES):\n",
        "        category_to_label[i] = c\n",
        "\n",
        "    # plot each category with a distinct label\n",
        "    fig = plt.figure(figsize = (12, 12))\n",
        "    ax = fig.add_subplot(111)\n",
        "    ax.set_title(\"t-SNE - \" + method_name.split('/')[-2])\n",
        "    for category, color in category_to_color.items():\n",
        "        mask = labels == category\n",
        "        ax.plot(tsne[mask, 0], tsne[mask, 1], 'o', \n",
        "                color=color, label=category_to_label[category])\n",
        "        \n",
        "    ax.legend(loc='best')\n",
        "    fig.savefig(method_name + 'tsne.png')\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def get_tsne_src_tgt(source_y_features, target_y_features, method_name):\n",
        "  source_y_features = torch.from_numpy(source_y_features)\n",
        "  target_y_features = torch.from_numpy(target_y_features)\n",
        "\n",
        "  tsne = TSNE(n_components=2, init=\"random\", learning_rate='auto').fit_transform(torch.cat([source_y_features, target_y_features]))\n",
        "\n",
        "  tsne_source, tsne_target = torch.split(torch.from_numpy(tsne), split_size_or_sections=source_y_features.shape[0])\n",
        "  fig = plt.figure()\n",
        "  ax = fig.add_subplot(111)\n",
        "  ax.set_title(\"t-SNE - Source & Target of \" + method_name.split('/')[-2])\n",
        "\n",
        "  ax.plot(tsne_source[:, 0], tsne_source[:, 1], 'o', color=\"blue\", label=\"source\")\n",
        "  ax.plot(tsne_target[:, 0], tsne_target[:, 1], 'o', color=\"red\", label=\"target\")\n",
        "      \n",
        "  ax.legend(loc='best')\n",
        "  fig.savefig(method_name + 'source_target_tsne.png')\n",
        "  plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEE8bKFMHWsr"
      },
      "source": [
        "#### Confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7cttQAZHWsr"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "def get_confusion_matrix(y_true, y_pred, method_name):\n",
        "    cf_matrix = confusion_matrix(y_true, y_pred, normalize='true')\n",
        "    df_cm = pd.DataFrame(cf_matrix, index = CLASSES,columns = CLASSES)\n",
        "    plt.figure(figsize = (13, 8))\n",
        "    plt.title(\"Confusion matrix - \" + method_name.split('/')[-2])\n",
        "    sns.heatmap(df_cm, annot=True)\n",
        "    plt.savefig(method_name + 'confusion_matrix.png')\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gL8vVrvNHWsr"
      },
      "source": [
        "#### Classification report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HnkDbloGHWss"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "\n",
        "def get_classification_report(y_trues, y_preds, method_name):\n",
        "    report = classification_report(y_trues, y_preds, target_names=CLASSES, output_dict=True)\n",
        "    pd.DataFrame(report).transpose().to_html(method_name + 'classification_report.html')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inil5tMrHWss"
      },
      "source": [
        "#### Generate and save results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ar1Jj9CpHWss"
      },
      "outputs": [],
      "source": [
        "def visualize_results(model, dl_source, dl_target, method_name):\n",
        "    if not os.path.exists(method_name):\n",
        "        os.makedirs(method_name, exist_ok = True)\n",
        "    \n",
        "    y_features_src, _, _ = get_predictions(model, dl_source)\n",
        "    y_features_tgt, y_trues, y_preds = get_predictions(model, dl_target)\n",
        "\n",
        "    get_tsne(y_trues, y_features_tgt, method_name)\n",
        "    get_tsne_src_tgt(y_features_src, y_features_tgt, method_name)\n",
        "    get_confusion_matrix(y_trues, y_preds, method_name)\n",
        "    get_classification_report(y_trues, y_preds, method_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-u3AC3trIMa"
      },
      "source": [
        "___\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LafDyaxOHWss"
      },
      "source": [
        "## 1Â° implementation: Baseline using ResNet18"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPDf4tK7HWss"
      },
      "source": [
        "The first implementation is the Baseline. We finetune a ResNet18 supervisedly on the source domain and evaluate it, as it is, on the target domain **without** any domain alignment strategy.\n",
        "\n",
        "We also compute the upper bound accuracy of each domain."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dg-JCN-cHWst"
      },
      "source": [
        "### Local constants for Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbfBYOFiHWst"
      },
      "outputs": [],
      "source": [
        "# training\n",
        "EPOCHS = 15\n",
        "OPTIMIZER = 'SGD'\n",
        "LR = 0.01\n",
        "WD = 0 # default PyTorch parameter\n",
        "MOMENTUM = 0.9\n",
        "GAMMA = 0.99    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiQm9ST8HWst"
      },
      "source": [
        "### Network architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6W3ReL_Iz4rV"
      },
      "source": [
        "We change the linear classifier with a new one. The output neurons are the number of the classes.\n",
        "\n",
        "We also defined a ``forward_features`` function to get the features extracted by the ResNet18 before applying the linear classifier, used when we want to plot the t-SNE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4MtVHSuHWst"
      },
      "outputs": [],
      "source": [
        "class ResNet18(torch.nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(ResNet18, self).__init__()\n",
        "        self.resnet = torchvision.models.resnet18(weights=WEIGHTS_RESNET18)\n",
        "        self.feature_extractor = torch.nn.Sequential(*(list(self.resnet.children())[:-1]))\n",
        "        # we get the last hidden layer to extract features before the classification. We feed these features to the Classification layer\n",
        "        # Reference: https://stackoverflow.com/questions/55083642/extract-features-from-last-hidden-layer-pytorch-resnet18\n",
        "        self.cls = torch.nn.Linear(512, num_classes) \n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.feature_extractor(x)\n",
        "        x = x.view(x.size(0), 512)\n",
        "        x = self.cls(x)\n",
        "        return x\n",
        "    \n",
        "    def forward_features(self, x):\n",
        "        x = self.feature_extractor(x)\n",
        "        x = x.view(x.size(0), 512)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWY_n-H1HWsu"
      },
      "source": [
        "### Training and Test steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHJL-pwiHWsu"
      },
      "outputs": [],
      "source": [
        "def training_step_baseline(model, optimizer, device, train_loader):\n",
        "  samples = 0.\n",
        "  total_loss = 0.\n",
        "  total_acc = 0.\n",
        "  cost_function = get_cost_function()\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
        "      \n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # forward pass\n",
        "      outputs = model(inputs)\n",
        "      loss = cost_function(outputs, labels)\n",
        "      pred = outputs.argmax(dim=1)\n",
        "\n",
        "      total_acc += pred.eq(labels).sum().item()\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      samples += inputs.shape[0]\n",
        "      total_loss += loss.item()\n",
        "  free_GPU_memory(inputs, labels)\n",
        "  \n",
        "  return {\"train/train_acc\":(total_acc/samples) * 100, \n",
        "          \"train/train_loss\": total_loss/samples} \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEnIPrwVHWsu"
      },
      "outputs": [],
      "source": [
        "def test_step_baseline(model, optimizer, device, test_loader):\n",
        "  samples = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "  cost_function = get_cost_function()\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        samples += inputs.shape[0]\n",
        "\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        loss = cost_function(outputs, targets)\n",
        "\n",
        "        pred = outputs.argmax(dim=1)\n",
        "\n",
        "        cumulative_loss += loss.item()\n",
        "        cumulative_accuracy += pred.eq(targets).sum().item()\n",
        "\n",
        "  free_GPU_memory(inputs, targets)\n",
        "\n",
        "  return {\"test/test_acc\": (cumulative_accuracy/samples) * 100, \n",
        "          \"test/test_loss\": cumulative_loss/samples}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHzA3-vkHWsu"
      },
      "source": [
        "### Declare the training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qt_GRkaCHWsu"
      },
      "outputs": [],
      "source": [
        "def training_baseline(data_source, data_target, wandb_setup):\n",
        "\n",
        "    config = wandb_setup.config\n",
        "    print('CONFIGS\\n', yaml.dump(config._items, default_flow_style=False)) # pretty print of configs\n",
        "\n",
        "    model = ResNet18(NUM_CLASSES).to(DEVICE)\n",
        "    optimizer = get_optimizer(model.parameters(), config.optimizer, config.lr, WD, MOMENTUM)\n",
        "    \n",
        "    best_acc = 0.\n",
        "    best_loss = 0.\n",
        "    \n",
        "    for e in range(config.epochs):\n",
        "        print(f'-- Epoch [{e+1}/{config.epochs}] --')\n",
        "        train_metrics = training_step_baseline(model, optimizer, DEVICE, data_source['train'])\n",
        "        test_metrics = test_step_baseline(model, optimizer, DEVICE, data_target['test'])\n",
        "        wandb.log({**train_metrics, **test_metrics})\n",
        "        print(f'Train -> \\tLoss:{train_metrics[\"train/train_loss\"]:.4f} \\tAccuracy: {train_metrics[\"train/train_acc\"]:.2f}')\n",
        "        print(f'Test -> \\tLoss:{test_metrics[\"test/test_loss\"]:.4f} \\tAccuracy: {test_metrics[\"test/test_acc\"]:.2f}')\n",
        "\n",
        "        if (best_acc < test_metrics[\"test/test_acc\"]):\n",
        "            best_model = copy.deepcopy(model)\n",
        "            best_acc = test_metrics[\"test/test_acc\"]\n",
        "            best_loss = test_metrics[\"test/test_loss\"]\n",
        "\n",
        "    os.makedirs(WEIGHTS_PATH + 'baseline/', exist_ok = True) \n",
        "    torch.save(best_model.state_dict(), WEIGHTS_PATH + 'baseline/' + config.name + '.pt')\n",
        "    \n",
        "    visualize_results(best_model, data_source['test'], data_target['test'], ASSETS_PATH + 'baseline/' + config.name + '/')\n",
        "\n",
        "    wandb.summary[\"test_best_acc\"] = best_acc\n",
        "    wandb.summary[\"test_best_loss\"] = best_loss\n",
        "    wandb.finish()\n",
        "\n",
        "    free_GPU_memory(model, best_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JL6S44AKHWsv"
      },
      "source": [
        "### Let's train the Baseline!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lKbLEZwHWsv"
      },
      "source": [
        "**Train**: Product <br>\n",
        "**Test**: Real World \n",
        "\n",
        "$P_{train} \\rightarrow RW_{test}$\n",
        "\n",
        "Best test accuracy $acc = 76\\% $"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YMJYExVHWsv"
      },
      "outputs": [],
      "source": [
        "NAME_RUN = \"baseline_P_to_RW\"\n",
        "config={\n",
        "    \"model\": \"ResNet18\",\n",
        "    \"version\": \"Source only\",\n",
        "    \"name\": NAME_RUN,\n",
        "    \"batch_size\": BATCH_SIZE,\n",
        "    \"epochs\": EPOCHS,\n",
        "    \"lr\": LR,\n",
        "    \"optimizer\": OPTIMIZER\n",
        "}\n",
        "\n",
        "training_baseline(product_data, rw_data, wandb.init(project=PROJECT_NAME, entity=ENTITY, name=NAME_RUN, mode=WANDB_MODE, config=config))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8Do-rSh0dbS"
      },
      "source": [
        "| t-SNE | Confusion matrix |\n",
        "|-|-|\n",
        "| ![tsne](https://drive.google.com/uc?export=view&id=1-X8dCHJjw8ogp4A-kgaVXSkj51JiLn65) | ![cm](https://drive.google.com/uc?export=view&id=1-Xj5EDf_BGJ9_J5g5HsPR8Mg_SXBtkMd)|\n",
        "\n",
        "<br>\n",
        "\n",
        "|Classification report |\n",
        "|-|\n",
        "\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>precision</th>\n",
        "      <th>recall</th>\n",
        "      <th>f1-score</th>\n",
        "      <th>support</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>backpack</th>\n",
        "      <td>0.516129</td>\n",
        "      <td>0.941176</td>\n",
        "      <td>0.666667</td>\n",
        "      <td>17.00</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>bookcase</th>\n",
        "      <td>0.928571</td>\n",
        "      <td>0.650000</td>\n",
        "      <td>0.764706</td>\n",
        "      <td>20.00</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>car jack</th>\n",
        "      <td>0.541667</td>\n",
        "      <td>0.866667</td>\n",
        "      <td>0.666667</td>\n",
        "      <td>15.00</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>comb</th>\n",
        "      <td>0.888889</td>\n",
        "      <td>0.727273</td>\n",
        "      <td>0.800000</td>\n",
        "      <td>22.00</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>crown</th>\n",
        "      <td>1.000000</td>\n",
        "      <td>0.952381</td>\n",
        "      <td>0.975610</td>\n",
        "      <td>21.00</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>file cabinet</th>\n",
        "      <td>0.608696</td>\n",
        "      <td>0.636364</td>\n",
        "      <td>0.622222</td>\n",
        "      <td>22.00</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>flat iron</th>\n",
        "      <td>0.764706</td>\n",
        "      <td>0.812500</td>\n",
        "      <td>0.787879</td>\n",
        "      <td>16.00</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>game controller</th>\n",
        "      <td>0.789474</td>\n",
        "      <td>0.714286</td>\n",
        "      <td>0.750000</td>\n",
        "      <td>21.00</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>glasses</th>\n",
        "      <td>1.000000</td>\n",
        "      <td>0.526316</td>\n",
        "      <td>0.689655</td>\n",
        "      <td>19.00</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>helicopter</th>\n",
        "      <td>1.000000</td>\n",
        "      <td>0.894737</td>\n",
        "      <td>0.944444</td>\n",
        "      <td>19.00</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>ice skates</th>\n",
        "      <td>0.708333</td>\n",
        "      <td>0.809524</td>\n",
        "      <td>0.755556</td>\n",
        "      <td>21.00</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>letter tray</th>\n",
        "      <td>0.677419</td>\n",
        "      <td>0.777778</td>\n",
        "      <td>0.724138</td>\n",
        "      <td>27.00</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>monitor</th>\n",
        "      <td>0.600000</td>\n",
        "      <td>0.750000</td>\n",
        "      <td>0.666667</td>\n",
        "      <td>20.00</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>mug</th>\n",
        "      <td>1.000000</td>\n",
        "      <td>0.833333</td>\n",
        "      <td>0.909091</td>\n",
        "      <td>24.00</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>network switch</th>\n",
        "      <td>0.909091</td>\n",
        "      <td>0.588235</td>\n",
        "      <td>0.714286</td>\n",
        "      <td>17.00</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>over-ear headphones</th>\n",
        "      <td>0.875000</td>\n",
        "      <td>0.823529</td>\n",
        "      <td>0.848485</td>\n",
        "      <td>17.00</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>pen</th>\n",
        "      <td>0.650000</td>\n",
        "      <td>0.764706</td>\n",
        "      <td>0.702703</td>\n",
        "      <td>17.00</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>purse</th>\n",
        "      <td>0.684211</td>\n",
        "      <td>0.565217</td>\n",
        "      <td>0.619048</td>\n",
        "      <td>23.00</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>stand mixer</th>\n",
        "      <td>0.833333</td>\n",
        "      <td>0.952381</td>\n",
        "      <td>0.888889</td>\n",
        "      <td>21.00</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>stroller</th>\n",
        "      <td>0.823529</td>\n",
        "      <td>0.666667</td>\n",
        "      <td>0.736842</td>\n",
        "      <td>21.00</td>\n",
        "    </tr>\n",
        "    <tr class=\"blank_row\">\n",
        "      <td colspan=\"6\"></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>accuracy</th>\n",
        "      <td>0.760000</td>\n",
        "      <td>0.760000</td>\n",
        "      <td>0.760000</td>\n",
        "      <td>0.76</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>macro avg</th>\n",
        "      <td>0.789952</td>\n",
        "      <td>0.762653</td>\n",
        "      <td>0.761678</td>\n",
        "      <td>400.00</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>weighted avg</th>\n",
        "      <td>0.793269</td>\n",
        "      <td>0.760000</td>\n",
        "      <td>0.763174</td>\n",
        "      <td>400.00</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "\n",
        "| Loss | Accuracy |\n",
        "|-|-|\n",
        "| ![loss](https://drive.google.com/uc?export=view&id=1HdIhyivMMctJ5nFGHiNkbI9uis2_VLFB) | ![accuracy](https://drive.google.com/uc?export=view&id=1orumdytadQMAgoXPDqLlE0IMj8rT2wpM)|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1LPfeYcLNIo"
      },
      "source": [
        "The results from the t-SNE and confusion matrix indicate that the extracted features are not sufficient for accurate classification. The overall accuracy is low, and the confusion matrix is very noisy. Additionally, the t-SNE results are not reliable.\n",
        "\n",
        "The \"bookcase\" category is particularly challenging to classify correctly, as it is often mistaken for a \"file cabinet\". This is likely due to similarities in patterns, such as vertical and horizontal lines, between the two objects. The Product domain images are of high quality, with controlled lighting and removed backgrounds, which may contribute to the model's poor performance when tested on Real World domain images. \n",
        "\n",
        "Furthermore, it appears that the model begins to overfit after 3 epochs.\n",
        "\n",
        "\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OSGMh8hHWsv"
      },
      "source": [
        "**Train**: Real World <br>\n",
        "**Test**: Product\n",
        "\n",
        "$RW_{train} \\rightarrow P_{test}$\n",
        "\n",
        "Best test accuracy $acc = 92\\% $"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z0ZLOvItHWsv"
      },
      "outputs": [],
      "source": [
        "NAME_RUN = \"baseline_RW_to_P\"\n",
        "config={\n",
        "    \"model\": \"ResNet18\",\n",
        "    \"version\": \"Source only\",\n",
        "    \"name\": NAME_RUN,\n",
        "    \"batch_size\": BATCH_SIZE,\n",
        "    \"epochs\": EPOCHS,\n",
        "    \"lr\": LR,\n",
        "    \"optimizer\": OPTIMIZER\n",
        "}\n",
        "\n",
        "training_baseline(rw_data, product_data, wandb.init(project=PROJECT_NAME, entity=ENTITY, name=NAME_RUN, mode=WANDB_MODE, config=config))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dbpvpw4A3eCk"
      },
      "source": [
        "| t-SNE | Confusion matrix |\n",
        "|-|-|\n",
        "| ![tsne](https://drive.google.com/uc?export=view&id=1-TtXK1Byfx_Z-CEAmBSw_X4jGWUwi_Pw) | ![cm](https://drive.google.com/uc?export=view&id=1-Ud9NVUq4bBPLXjLYrvA2OqqXKnrJQsx)|\n",
        "\n",
        "<br>\n",
        "\n",
        "|Classification report |\n",
        "|-|\n",
        "\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>precision</th>\n",
        "      <th>recall</th>\n",
        "      <th>f1-score</th>\n",
        "      <th>support</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>backpack</th>\n",
        "      <td>0.964286</td>\n",
        "      <td>0.931034</td>\n",
        "      <td>0.947368</td>\n",
        "      <td>29.00</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>bookcase</th>\n",
        "      <td>0.826087</td>\n",
        "      <td>0.904762</td>\n",
        "      <td>0.863636</td>\n",
        "      <td>21.00</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>car jack</th>\n",
        "      <td>1.000000</td>\n",
        "      <td>0.764706</td>\n",
        "      <td>0.866667</td>\n",
        "      <td>17.00</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>comb</th>\n",
        "      <td>0.826087</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>0.904762</td>\n",
        "      <td>19.00</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>crown</th>\n",
        "      <td>1.000000</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>20.00</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>file cabinet</th>\n",
        "      <td>0.875000</td>\n",
        "      <td>0.777778</td>\n",
        "      <td>0.823529</td>\n",
        "      <td>18.00</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>flat iron</th>\n",
        "      <td>0.789474</td>\n",
        "      <td>0.937500</td>\n",
        "      <td>0.857143</td>\n",
        "      <td>16.00</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>game controller</th>\n",
        "      <td>1.000000</td>\n",
        "      <td>0.875000</td>\n",
        "      <td>0.933333</td>\n",
        "      <td>24.00</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>glasses</th>\n",
        "      <td>0.950000</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>0.974359</td>\n",
        "      <td>19.00</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>helicopter</th>\n",
        "      <td>0.894737</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>0.944444</td>\n",
        "      <td>17.00</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>ice skates</th>\n",
        "      <td>1.000000</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>19.00</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>letter tray</th>\n",
        "      <td>0.933333</td>\n",
        "      <td>0.875000</td>\n",
        "      <td>0.903226</td>\n",
        "      <td>16.00</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>monitor</th>\n",
        "      <td>1.000000</td>\n",
        "      <td>0.900000</td>\n",
        "      <td>0.947368</td>\n",
        "      <td>20.00</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>mug</th>\n",
        "      <td>0.944444</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>0.971429</td>\n",
        "      <td>17.00</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>network switch</th>\n",
        "      <td>0.960000</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>0.979592</td>\n",
        "      <td>24.00</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>over-ear headphones</th>\n",
        "      <td>0.789474</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>0.882353</td>\n",
        "      <td>15.00</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>pen</th>\n",
        "      <td>0.923077</td>\n",
        "      <td>0.827586</td>\n",
        "      <td>0.872727</td>\n",
        "      <td>29.00</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>purse</th>\n",
        "      <td>0.857143</td>\n",
        "      <td>0.857143</td>\n",
        "      <td>0.857143</td>\n",
        "      <td>21.00</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>stand mixer</th>\n",
        "      <td>0.937500</td>\n",
        "      <td>0.789474</td>\n",
        "      <td>0.857143</td>\n",
        "      <td>19.00</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>stroller</th>\n",
        "      <td>0.952381</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>0.975610</td>\n",
        "      <td>20.00</td>\n",
        "    </tr>\n",
        "    <tr class=\"blank_row\">\n",
        "      <td colspan=\"6\"></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>accuracy</th>\n",
        "      <td>0.920000</td>\n",
        "      <td>0.920000</td>\n",
        "      <td>0.920000</td>\n",
        "      <td>0.92</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>macro avg</th>\n",
        "      <td>0.921151</td>\n",
        "      <td>0.921999</td>\n",
        "      <td>0.918092</td>\n",
        "      <td>400.00</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>weighted avg</th>\n",
        "      <td>0.925376</td>\n",
        "      <td>0.920000</td>\n",
        "      <td>0.919515</td>\n",
        "      <td>400.00</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "\n",
        "| Loss | Accuracy |\n",
        "|-|-|\n",
        "| ![loss](https://drive.google.com/uc?export=view&id=10wKBqiLWtr1spSJR2tjjdpz0xp6MqVM_) | ![accuracy](https://drive.google.com/uc?export=view&id=1AP49Fd4L4ZpFCbrzahE7uYB9bkv_egYJ)|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGhWw922L04t"
      },
      "source": [
        "In contrast to the previous experiment, we have observed an increase in accuracy when training on Real World images and testing on Product images. \n",
        "\n",
        "The t-SNE and confusion matrix results show a clear improvement in classification accuracy. Some object categories are classified completely correct. This improvement can be attributed to the Real World domain images providing a better generalization for the model, resulting in a higher accuracy when tested on Product images. \n",
        "\n",
        "As in the previous experiment, we also observe that the model tends to overfit after 3-4 epochs.\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJX59XgjHWsv"
      },
      "source": [
        "**Train**: Product <br>\n",
        "**Test**: Product \n",
        "\n",
        "$P_{train} \\rightarrow P_{test}$\n",
        "\n",
        "Upper bound accuracy $acc = 96\\% $"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCUQOhamHWsw"
      },
      "outputs": [],
      "source": [
        "NAME_RUN = \"baseline_P_to_P\"\n",
        "config={\n",
        "    \"model\": \"ResNet18\",\n",
        "    \"version\": \"Source only\",\n",
        "    \"name\": NAME_RUN,\n",
        "    \"batch_size\": BATCH_SIZE,\n",
        "    \"epochs\": EPOCHS,\n",
        "    \"lr\": LR,\n",
        "    \"optimizer\": OPTIMIZER\n",
        "}\n",
        "\n",
        "training_baseline(product_data, product_data, wandb.init(project=PROJECT_NAME, entity=ENTITY, name=NAME_RUN, mode=WANDB_MODE, config=config))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LAkDx6G37fA"
      },
      "source": [
        "| t-SNE | Confusion matrix | \n",
        "|-|-|\n",
        "| ![tsne](https://drive.google.com/uc?export=view&id=1-R2SWSIcg2CzrbKKvYds89aAJJzAbFYc) | ![cm](https://drive.google.com/uc?export=view&id=1-S60LG_CV88_jnU4Org6x_j7ieOCAGa-) |\n",
        "\n",
        "<br>\n",
        "\n",
        "|Classification report |\n",
        "|-|\n",
        "\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>precision</th>\n",
        "      <th>recall</th>\n",
        "      <th>f1-score</th>\n",
        "      <th>support</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>backpack</th>\n",
        "      <td>0.966667</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>0.983051</td>\n",
        "      <td>29.000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>bookcase</th>\n",
        "      <td>0.869565</td>\n",
        "      <td>0.952381</td>\n",
        "      <td>0.909091</td>\n",
        "      <td>21.000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>car jack</th>\n",
        "      <td>0.937500</td>\n",
        "      <td>0.882353</td>\n",
        "      <td>0.909091</td>\n",
        "      <td>17.000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>comb</th>\n",
        "      <td>0.904762</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>0.950000</td>\n",
        "      <td>19.000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>crown</th>\n",
        "      <td>1.000000</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>20.000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>file cabinet</th>\n",
        "      <td>1.000000</td>\n",
        "      <td>0.833333</td>\n",
        "      <td>0.909091</td>\n",
        "      <td>18.000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>flat iron</th>\n",
        "      <td>0.941176</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>0.969697</td>\n",
        "      <td>16.000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>game controller</th>\n",
        "      <td>0.954545</td>\n",
        "      <td>0.875000</td>\n",
        "      <td>0.913043</td>\n",
        "      <td>24.000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>glasses</th>\n",
        "      <td>1.000000</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>19.000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>helicopter</th>\n",
        "      <td>0.894737</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>0.944444</td>\n",
        "      <td>17.000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>ice skates</th>\n",
        "      <td>1.000000</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>19.000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>letter tray</th>\n",
        "      <td>0.937500</td>\n",
        "      <td>0.937500</td>\n",
        "      <td>0.937500</td>\n",
        "      <td>16.000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>monitor</th>\n",
        "      <td>1.000000</td>\n",
        "      <td>0.900000</td>\n",
        "      <td>0.947368</td>\n",
        "      <td>20.000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>mug</th>\n",
        "      <td>0.944444</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>0.971429</td>\n",
        "      <td>17.000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>network switch</th>\n",
        "      <td>1.000000</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>24.000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>over-ear headphones</th>\n",
        "      <td>1.000000</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>15.000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>pen</th>\n",
        "      <td>0.862069</td>\n",
        "      <td>0.862069</td>\n",
        "      <td>0.862069</td>\n",
        "      <td>29.000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>purse</th>\n",
        "      <td>1.000000</td>\n",
        "      <td>0.904762</td>\n",
        "      <td>0.950000</td>\n",
        "      <td>21.000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>stand mixer</th>\n",
        "      <td>0.950000</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>0.974359</td>\n",
        "      <td>19.000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>stroller</th>\n",
        "      <td>1.000000</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>20.000</td>\n",
        "    </tr>\n",
        "    <tr class=\"blank_row\">\n",
        "      <td colspan=\"6\"></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>accuracy</th>\n",
        "      <td>0.955000</td>\n",
        "      <td>0.955000</td>\n",
        "      <td>0.955000</td>\n",
        "      <td>0.955</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>macro avg</th>\n",
        "      <td>0.958148</td>\n",
        "      <td>0.957370</td>\n",
        "      <td>0.956512</td>\n",
        "      <td>400.000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>weighted avg</th>\n",
        "      <td>0.956765</td>\n",
        "      <td>0.955000</td>\n",
        "      <td>0.954689</td>\n",
        "      <td>400.000</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHg0FUxfL3Ie"
      },
      "source": [
        "\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tc2W-Sn1HWsw"
      },
      "source": [
        "**Train**: Real World <br>\n",
        "**Test**: Real World \n",
        "\n",
        "$RW_{train} \\rightarrow RW_{test}$\n",
        "\n",
        "Upper bound accuracy $acc = 91\\% $"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlvSvtzLHWsw"
      },
      "outputs": [],
      "source": [
        "NAME_RUN = \"baseline_RW_to_RW\"\n",
        "config={\n",
        "        \"model\": \"ResNet18\",\n",
        "        \"version\": \"Source only\",\n",
        "        \"name\": NAME_RUN,\n",
        "        \"batch_size\": BATCH_SIZE,\n",
        "        \"epochs\": EPOCHS,\n",
        "        \"lr\": LR,\n",
        "        \"optimizer\": OPTIMIZER\n",
        "    }\n",
        "\n",
        "training_baseline(rw_data, rw_data, wandb.init(project=PROJECT_NAME, entity=ENTITY, name=NAME_RUN, mode=WANDB_MODE, config=config))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzAiJaKw99yw"
      },
      "source": [
        "| t-SNE | Confusion matrix | \n",
        "|-|-|\n",
        "| ![tsne](https://drive.google.com/uc?export=view&id=1-VHZFUVkQniNZbZ2UTdxvp_OCH5Z1yQw) | ![cm](https://drive.google.com/uc?export=view&id=1-WIzxpDSEtDhVXJlcEama4OyrSpZXxDz) |\n",
        "\n",
        "<br>\n",
        "\n",
        "|Classification report |\n",
        "|-|\n",
        "\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>precision</th>\n",
        "      <th>recall</th>\n",
        "      <th>f1-score</th>\n",
        "      <th>support</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>backpack</th>\n",
        "      <td>0.809524</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>0.894737</td>\n",
        "      <td>17.000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>bookcase</th>\n",
        "      <td>0.941176</td>\n",
        "      <td>0.800000</td>\n",
        "      <td>0.864865</td>\n",
        "      <td>20.000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>car jack</th>\n",
        "      <td>1.000000</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>15.000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>comb</th>\n",
        "      <td>0.909091</td>\n",
        "      <td>0.909091</td>\n",
        "      <td>0.909091</td>\n",
        "      <td>22.000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>crown</th>\n",
        "      <td>0.954545</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>0.976744</td>\n",
        "      <td>21.000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>file cabinet</th>\n",
        "      <td>0.818182</td>\n",
        "      <td>0.818182</td>\n",
        "      <td>0.818182</td>\n",
        "      <td>22.000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>flat iron</th>\n",
        "      <td>0.937500</td>\n",
        "      <td>0.937500</td>\n",
        "      <td>0.937500</td>\n",
        "      <td>16.000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>game controller</th>\n",
        "      <td>0.909091</td>\n",
        "      <td>0.952381</td>\n",
        "      <td>0.930233</td>\n",
        "      <td>21.000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>glasses</th>\n",
        "      <td>0.947368</td>\n",
        "      <td>0.947368</td>\n",
        "      <td>0.947368</td>\n",
        "      <td>19.000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>helicopter</th>\n",
        "      <td>1.000000</td>\n",
        "      <td>0.894737</td>\n",
        "      <td>0.944444</td>\n",
        "      <td>19.000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>ice skates</th>\n",
        "      <td>1.000000</td>\n",
        "      <td>0.761905</td>\n",
        "      <td>0.864865</td>\n",
        "      <td>21.000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>letter tray</th>\n",
        "      <td>0.793103</td>\n",
        "      <td>0.851852</td>\n",
        "      <td>0.821429</td>\n",
        "      <td>27.000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>monitor</th>\n",
        "      <td>0.904762</td>\n",
        "      <td>0.950000</td>\n",
        "      <td>0.926829</td>\n",
        "      <td>20.000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>mug</th>\n",
        "      <td>0.916667</td>\n",
        "      <td>0.916667</td>\n",
        "      <td>0.916667</td>\n",
        "      <td>24.000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>network switch</th>\n",
        "      <td>0.809524</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>0.894737</td>\n",
        "      <td>17.000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>over-ear headphones</th>\n",
        "      <td>0.941176</td>\n",
        "      <td>0.941176</td>\n",
        "      <td>0.941176</td>\n",
        "      <td>17.000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>pen</th>\n",
        "      <td>0.941176</td>\n",
        "      <td>0.941176</td>\n",
        "      <td>0.941176</td>\n",
        "      <td>17.000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>purse</th>\n",
        "      <td>0.882353</td>\n",
        "      <td>0.652174</td>\n",
        "      <td>0.750000</td>\n",
        "      <td>23.000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>stand mixer</th>\n",
        "      <td>0.913043</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>0.954545</td>\n",
        "      <td>21.000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>stroller</th>\n",
        "      <td>0.909091</td>\n",
        "      <td>0.952381</td>\n",
        "      <td>0.930233</td>\n",
        "      <td>21.000</td>\n",
        "    </tr>\n",
        "    <tr class=\"blank_row\">\n",
        "      <td colspan=\"6\"></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>accuracy</th>\n",
        "      <td>0.905000</td>\n",
        "      <td>0.905000</td>\n",
        "      <td>0.905000</td>\n",
        "      <td>0.905</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>macro avg</th>\n",
        "      <td>0.911869</td>\n",
        "      <td>0.911330</td>\n",
        "      <td>0.908241</td>\n",
        "      <td>400.000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>weighted avg</th>\n",
        "      <td>0.908879</td>\n",
        "      <td>0.905000</td>\n",
        "      <td>0.903542</td>\n",
        "      <td>400.000</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsUVL_wEL5r2"
      },
      "source": [
        "\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkhNVHj-HWsw"
      },
      "source": [
        "### Observations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IquJI2F_0ujF"
      },
      "source": [
        "The test accuracies obtained by finetuning a pre-trained ResNet18 model without using any unsupervised domain adaptation framework are already quite high. This is likely due to the fact that ResNet18 is a model that has been trained on a large dataset of images (ImageNet in this case) and is able to generalize well, allowing for the transfer of learned features to other image datasets such as Adaptiope. \n",
        "\n",
        "The experiments show that training on Real World domain and testing on Product domain leads to better results. \n",
        "\n",
        "Additionally, it is observed that the model tends to overfit after 3-4 epochs in all experiments.\n",
        "\n",
        "We report in a table the results obtained by the Baseline implementation: \n",
        "\n",
        "|       | Baseline | \n",
        "|-------|----------|\n",
        "| $P\\rightarrow RW$ | $76\\%$ |\n",
        "| $RW\\rightarrow P$ | $92\\%$ |\n",
        "| $P\\rightarrow P$  | $96\\%$ |\n",
        "| $RW\\rightarrow RW$| $91\\%$ |\n",
        "\n",
        "___\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urKkEklkXbwR"
      },
      "source": [
        "## 2Â° implementation: Maximum Classifier Discrepancy for UDA (MCD_DA)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XzyIGYn43LTR"
      },
      "source": [
        "To improve the baseline we decided to implement the [Maximum Classifier Discrepancy for Unsupervised Domain Adaptation](https://arxiv.org/abs/1712.02560) approach proposed by Saito et. al. in 2018.\n",
        "\n",
        "Many UDA algorithms, particularly those for training neural networks, attempt to match the distribution of the source features with that of the target without considering the category of the sample. The method utilize two players to align distributions in an adversarial manner: domain classifier and feature generator. Source and target samples are input to the same feature generator.\n",
        "The authors pointed out that the previous method assumes that target features are aligned with the source samples, thus classify them correctly by the object classifier.\n",
        "\n",
        "The proposed method tries to overcome two main problems:\n",
        "-  previous approaches should fail to extract discriminative features because they do not consider the relationship between target samples and the object decision boundary when aligning distributions;\n",
        "- the generator can generate ambiguous features near the boundary because it simply tries to make the two distributions similar.\n",
        "\n",
        "This approach, instead, attempts to align distributions of source and target by utilizing the object decision boundaries. They propose to maximize the discrepancy between two classifiersâ outputs to detect target samples that are far from the support of the source. A feature generator learns to generate target features near the support to minimize the discrepancy.\n",
        "\n",
        "We can clearly see from the figure below what the authors proposed and wanted to achieve with this approach:\n",
        "- Left. Previous methods try to match different distributions by mimicing the domain classifier without considering the decision boundaries;\n",
        "- Right. The proposed method tries to detect target samples outside the support of the source distribution using an object classifier.\n",
        "\n",
        "![figure_1](https://drive.google.com/uc?export=view&id=1vXFsoSgyCa-tVA1qFSRVS-xZwAbD3qyA)\n",
        "\n",
        "Now we give **more details** of the overall process. \n",
        "\n",
        "First of all, we want to remark that we have access to a labeled source image $x_s$ and a corresponding label $y_s$ drawn from a set of labeled source images ${X_s, Y_s}$, as well as an unlabeled target $x_t$ drawn from unlabeled target images $X_t$.\n",
        "\n",
        "The forward pipeline is the following:\n",
        "- The generator $G$ extracts features from the inputs $\\mathbf{x_s}$ or $\\mathbf{x_t}$;\n",
        "- Two classifiers take the features from the generator and try to classify them into $K$ classes (in this case 20 classes);\n",
        "- The output of each discriminator is a K-dimensional vector of logits, where we apply the softmax function to the vector to obtain class probabilities. We use the annotation $p_1(\\mathbf{y|x})$ and $p_2(\\mathbf{y|x})$ respectively for the output vector from $F_1$ and $F_2$ with input $x$.\n",
        "\n",
        "The goal of the method is to classify, using the two classifiers, the source samples correctly and, simultaneously, they are trained to detect the target samples that are far from the support of the source. In this way we are trying to align source and target features and consider the relationship between class boundaries and target samples. This means that samples far from the support do not have discriminative features because they are not clearly categorized into some classes. To identify these misclassified target samples, we utilize the disagreement of the two classifiers on the prediction for target samples. In this way the two classifiers can be treated as discriminators. The figure below can help us to understand the idea:\n",
        "- Leftmost side. Two classifiers, inizialized differently, are assumed to classify source samples correctly (realistic assumption because we train supervisely the network with the source dataset). The *Discrepancy region* is likely to misclassify target samples;\n",
        "- Right side. The generator is trained to output features that simply tries to make the two distributions similar by generating target features near the support, while considering classifiers' output for target samples. In this way the generator avoids generating features outside the support of the source. To achieve this we use the term *discrepancy* $d\\left(p_1(\\mathbf{y|x_t}), p_2(\\mathbf{y|x_t})\\right)$, to measure the disagreement of the two classifiers on their predictions.\n",
        "\n",
        "![overview](https://mil-tokyo.github.io/MCD_DA/overview.png)\n",
        "\n",
        "At the end, the proposed method trains discriminators $F_1$ and $F_2$ to maximize the discrepancy (*Maximize Discrepancy* in the figure) given target features, and then train the generator to fool the discrimators by minimizing the discrepancy (*Minimize Discrepancy* in the figure). The goal is to obtains features in which the support of the target is included by that of the source (*Obtained Distributions* in the figure). This allows the generator to generate discriminative features for target samples because it considers the relationship between the decision boundaries and target samples. This training is achieved in an adversarial manner.\n",
        "\n",
        "Each step of the paper will be discussed in details in the following sections."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4OgMlxWHWsx"
      },
      "source": [
        "### Local constants for MCD_DA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIFpUHDzHWsx"
      },
      "outputs": [],
      "source": [
        "# training\n",
        "EPOCHS = 15\n",
        "OPTIMIZER = 'Adam' \n",
        "LR = 0.001\n",
        "WD = 5e-4\n",
        "MOMENTUM = 0.9\n",
        "GAMMA = 0.99\n",
        "NUM_K = 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmH-0bdnHWsx"
      },
      "source": [
        "### Loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmiVavxPHWsx"
      },
      "source": [
        "The loss function applied for the unsupervised DA task is the discrepancy loss $\\mathcal{L_d}$ used and defined in the paper as the absolute values of the difference beween the two classifiers' probabilistic outputs:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathcal{L_d}\\left(p_1, p_2\\right)=\\frac{1}{K} \\sum_{k=1}^K\\left|p_{1 _k}-p_{2_k}\\right|\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "where $p_{1_k}$ and $p_{2_k}$ denote probability output of $p_1$ and $p_2$ for class $k$ respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNFn08JxHWsy"
      },
      "outputs": [],
      "source": [
        "def get_discrepancy_loss(p1,p2):\n",
        "  d_loss = torch.mean(torch.abs(F.softmax(p1, dim=1) - F.softmax(p2, dim=1)))\n",
        "  return d_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRV9CLN0HWsy"
      },
      "source": [
        "### Network architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIHzhfp9g__d"
      },
      "source": [
        "The network is separated into two modules:\n",
        "- Generator ($G$), the feature generator is a ResNet18 without the classification layer. The output will be a 512 features vector;\n",
        "- Discriminator ($F_1$,$F_2$), the two classifiers are a feed-forward network. The output will be a 20 logits vector corresponding to the probability of each class of the image in input.\n",
        "\n",
        "In the Discriminator Class we have also added a ``forward_features`` function to extract features before the classification layer. These features are useful for the visualization using T-SNE.\n",
        "\n",
        "> We have not fixed any random seed for the networks, so the classifiers' weights are initizialized randomly from PyTorch. In this way we follow what the paper wants for the two classifiers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTCHkD8UEaKj"
      },
      "outputs": [],
      "source": [
        "class Generator(torch.nn.Module):\n",
        "    def __init__(self): \n",
        "        super(Generator, self).__init__()\n",
        "        self.resnet = torchvision.models.resnet18(weights=WEIGHTS_RESNET18)\n",
        "        self.feature_extractor = torch.nn.Sequential(*(list(self.resnet.children())[:-1])) \n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.feature_extractor(x)\n",
        "        x = x.view(x.size(0), 512)\n",
        "        return x\n",
        "\n",
        "class Discriminator(torch.nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.dropout = torch.nn.Dropout(p=0.5)\n",
        "        self.bn = torch.nn.BatchNorm1d(256,affine=True)\n",
        "        self.relu = torch.nn.ReLU(inplace=True)\n",
        "        self.fc1 = torch.nn.Linear(512, 256)\n",
        "        self.fc2 = torch.nn.Linear(256, 256)\n",
        "        self.cls = torch.nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.bn(self.fc1(self.dropout(x))))\n",
        "        x = self.relu(self.bn(self.fc2(self.dropout(x))))\n",
        "        x = self.cls(x)\n",
        "        return x\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        x = self.relu(self.bn(self.fc1(self.dropout(x))))\n",
        "        x = self.relu(self.bn(self.fc2(self.dropout(x))))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwdN0exeHWsy"
      },
      "source": [
        "### Training step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOe45kBpjB_1"
      },
      "source": [
        "For the training process, the authors proposed 3 steps:\n",
        "- **Step A**. We train both classifiers $F_1$ and $F_2$ and generator $G$ to classify the source samples correctly. We compute the cross-entropy loss between the outputs of the classifiers and the corresponding source labels;\n",
        "- **Step B**. We train both classifiers as a discriminator fixing the generator. We compute the cross-entropy loss of the outputs and labels source, and the discrepancy loss between the output of target images from the classifiers. In this step $F_1$ and $F_2$ try to maximize the discrepancy loss;\n",
        "- **Step C**. We train the generator to mimimize the discrepancy fixing the classifiers. The parameter ``num_k`` indicates the number of times we repeat this step for the same batch. This term denotes the trade-off between the generator and the classifiers.\n",
        "\n",
        "These three steps are repeated across the entire training step and for every batch. The order of the steps is not revelant, according to the authors. We deploy the solution in the order presented by the paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qg_oUWoeHWsz"
      },
      "outputs": [],
      "source": [
        "def reset_grad(opt):\n",
        "    opt[\"gen\"].zero_grad()\n",
        "    opt[\"clf1\"].zero_grad()\n",
        "    opt[\"clf2\"].zero_grad()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHdztS1iHWsz"
      },
      "source": [
        "#### Step A"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obsEZg_vcj4Q"
      },
      "source": [
        "We train the classifier $F_1$ and $F_2$ and the generator $G$ in supervised mode to classify the source samples correctly. We compute the cross-entropy loss between the outputs of the classifiers and the corresponding source labels, then we backpropagate the error. We train the network to minimize the cross entropy loss. The objective is as follow:\n",
        "\n",
        "$$\\begin{aligned}\n",
        "\\min_{G, F_1, F_2} \\mathcal{L_{ce}}\\left(X_s, Y_s\\right)\n",
        "\\end{aligned}$$\n",
        "\n",
        "where $\\mathcal{L_{ce}}$ is the Cross entropy loss declared in section *Loss function* of this notebook, $X_s$ and $Y_s$ are the set of labeled source images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dov3zLbOHWsz"
      },
      "outputs": [],
      "source": [
        "def stepA(net, opt, img_source, label_source, cost_function):\n",
        "\n",
        "      # Forward pass\n",
        "      feat_s    = net[\"gen\"](img_source)\n",
        "      output_s1 = net[\"clf1\"](feat_s)\n",
        "      output_s2 = net[\"clf2\"](feat_s)\n",
        "\n",
        "      # Apply the losses\n",
        "      loss_s1 = cost_function(output_s1, label_source)\n",
        "      loss_s2 = cost_function(output_s2, label_source)\n",
        "      loss_s = loss_s1 + loss_s2\n",
        "\n",
        "      # Backward pass\n",
        "      loss_s.backward()\n",
        "\n",
        "      # Update parameters\n",
        "      opt[\"gen\"].step()\n",
        "      opt[\"clf1\"].step()\n",
        "      opt[\"clf2\"].step()\n",
        "\n",
        "      # Reset grad\n",
        "      reset_grad(opt)\n",
        "      \n",
        "      return [loss_s1, loss_s2]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ufi3K1-HWsz"
      },
      "source": [
        "#### Step B "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZ136j73b_dY"
      },
      "source": [
        "We train both classifiers $F_1$ and $F_2$ as a discriminator fixing the generator $G$. We compute the cross-entropy loss of the outputs and labels source, and the discrepancy loss between the outputs of target images from the classifiers. By training the classifiers to increase the discrepancy, they can detect the target samples excluded by the support of the source. In this step $F_1$ and $F_2$ try to maximize the discrepancy loss.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1j18HhFRW22YXKTAOGqHvQ3BGukBIDu94\" alt=\"stepB\" width=\"500\">\n",
        "\n",
        "The objective is as follows:\n",
        "\n",
        "$$\\begin{aligned}\n",
        "\\min_{F_1, F_2} [\\mathcal{L_{ce_{F_1}}}\\left(X_s, Y_s\\right) + \\mathcal{L_{ce_{F_2}}}\\left(X_s, Y_s\\right)] - \\mathcal{L_{d}}(X_t)\n",
        "\\end{aligned}$$\n",
        "\n",
        "where $\\mathcal{L_{ce_{F_1}}}$ is the cross entropy loss computed by the classifier $F_1$, $\\mathcal{L_{ce_{F_2}}}$ is the cross entropy loss computed by the classifier $F_2$, and $\\mathcal{L_{d}}$ is the discrepancy loss and $X_t$ is the target sample.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sbb1te48HWsz"
      },
      "outputs": [],
      "source": [
        "def stepB(net, opt, img_source, label_source, img_target, cost_function):\n",
        "\n",
        "    # Forward pass\n",
        "    feat_s    = net[\"gen\"](img_source)\n",
        "    output_s1 = net[\"clf1\"](feat_s)\n",
        "    output_s2 = net[\"clf2\"](feat_s)\n",
        "\n",
        "    feat_t    = net[\"gen\"](img_target)\n",
        "    output_t1 = net[\"clf1\"](feat_t)\n",
        "    output_t2 = net[\"clf2\"](feat_t)\n",
        "\n",
        "    # Apply the losses\n",
        "    loss_s1 = cost_function(output_s1,label_source)\n",
        "    loss_s2 = cost_function(output_s2,label_source)\n",
        "    loss_disc = get_discrepancy_loss(output_t1, output_t2)\n",
        "\n",
        "    loss = (loss_s1 + loss_s2) - loss_disc\n",
        "    loss.backward()\n",
        "\n",
        "    # We don't step the generator because we keep the weights fixed\n",
        "    opt[\"clf1\"].step() \n",
        "    opt[\"clf2\"].step()\n",
        "\n",
        "    # Reset the optimizers\n",
        "    reset_grad(opt)\n",
        "\n",
        "    return [loss_s1, loss_s2, loss_disc]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCjVEnK2HWs4"
      },
      "source": [
        "#### Step C"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PD8nQu6Fc4Un"
      },
      "source": [
        "We train the generator to mimimize the discrepancy fixing the classifiers. The parameter ``num_k`` indicates the number of times we repeat this step for the same batch. This term denotes the trade-off between the generator and the classifiers.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1DgaxB2wVHxp51NoHJSFgGX369zDQeFok\" alt=\"stepC\" width=\"500\">\n",
        "\n",
        "The objective is as follows:\n",
        "$$\\begin{aligned}\n",
        "\\min_{G} \\mathcal{L_{d}}(X_t)\n",
        "\\end{aligned}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOAwjQmcHWs4"
      },
      "outputs": [],
      "source": [
        "def stepC(net, opt, img_target, cost_function, num_k):\n",
        "      loss_disc = None\n",
        "      for i in range(num_k):\n",
        "        # Forward pass\n",
        "        feat_t    = net[\"gen\"](img_target)\n",
        "        output_t1 = net[\"clf1\"](feat_t)\n",
        "        output_t2 = net[\"clf2\"](feat_t)\n",
        "\n",
        "        loss_disc = get_discrepancy_loss(output_t1, output_t2)\n",
        "        loss_disc.backward()\n",
        "\n",
        "        # We don't step the classifiers because we keep the weights fixed\n",
        "        opt[\"gen\"].step()\n",
        "\n",
        "        # Reset the optimizers\n",
        "        reset_grad(opt)\n",
        "\n",
        "      return [loss_disc]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DUPBAsmHWs4"
      },
      "source": [
        "In the following function ``training_step_MCD_DA``, we train the three networks on all the training set batches by using the three steps presented above.\n",
        "\n",
        "Following what we have seen in the course lab, we implemented ``try`` and ``except`` condition when we reach the end of the target set batch, in order to restart it when it ends.\n",
        "\n",
        "It is worth noticing that we do not extract the target labels from the target batch. This confirms that we are working in unsupervised mode with the target samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIb7I2BUTXfY"
      },
      "outputs": [],
      "source": [
        "def training_step_MCD_DA(net, opt, scheduler, cost_function, source_train_loader, target_train_loader):\n",
        "  source_samples = 0.\n",
        "  target_samples = 0.\n",
        "  cumulative_ce_loss  = np.zeros(2)\n",
        "  cumulative_discrepancy = 0.\n",
        "\n",
        "  target_iter = iter(target_train_loader)\n",
        "\n",
        "  net[\"gen\"].train()\n",
        "  net[\"clf1\"].train()\n",
        "  net[\"clf2\"].train()\n",
        "\n",
        "  for batch_idx, (img_source, label_source) in enumerate(source_train_loader):\n",
        "\n",
        "      # get target data. If the target iterator reaches the end, restart it\n",
        "      try:\n",
        "        img_target, _ = next(target_iter)\n",
        "      except:\n",
        "        target_iter = iter(target_train_loader)\n",
        "        img_target, _ = next(target_iter)\n",
        "\n",
        "      img_source = img_source.to(DEVICE)\n",
        "      label_source = label_source.to(DEVICE)\n",
        "      img_target = img_target.to(DEVICE)\n",
        "\n",
        "      loss_stepA = stepA(net, opt, img_source, label_source, cost_function)\n",
        "      loss_stepB = stepB(net, opt, img_source, label_source, img_target, cost_function)\n",
        "      loss_stepC = stepC(net, opt, img_target, cost_function, num_k = NUM_K)\n",
        "      \n",
        "      source_samples += img_source.shape[0]\n",
        "      target_samples += img_target.shape[0]\n",
        "\n",
        "      cumulative_ce_loss[0]  += loss_stepB[0].item()\n",
        "      cumulative_ce_loss[1]  += loss_stepB[1].item()\n",
        "      cumulative_discrepancy += loss_stepC[0].item()\n",
        "\n",
        "  scheduler['clf1'].step()\n",
        "  scheduler['clf2'].step()\n",
        "  \n",
        "  free_GPU_memory(img_source, label_source, img_target)\n",
        "\n",
        "  return {\"train/train_loss1\": cumulative_ce_loss[0]/source_samples, \n",
        "          \"train/train_loss2\": cumulative_ce_loss[1]/source_samples, \n",
        "          \"train/train_disc_loss\": cumulative_discrepancy/target_samples}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WumHxdC4HWs5"
      },
      "source": [
        "### Test step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Q38zsMaP3Pz"
      },
      "source": [
        "The **ensemble accuracy** refers to the accuracy of a group of models when used together to make predictions. In this test step we compute it by sum the outputs logits of the two classifiers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCdKUf4aHioG"
      },
      "outputs": [],
      "source": [
        "def test_step_MCD_DA(net, cost_function, target_test_loader):\n",
        "  samples = 0.\n",
        "  test_loss = 0.\n",
        "  cumulative_accuracy = np.zeros(3)\n",
        "\n",
        "  net[\"gen\"].eval()\n",
        "  net[\"clf1\"].eval()\n",
        "  net[\"clf2\"].eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, (img, label) in enumerate(target_test_loader):\n",
        "      \n",
        "      img = img.to(DEVICE)\n",
        "      label = label.to(DEVICE)\n",
        "        \n",
        "      # Forward pass\n",
        "      feat      = net[\"gen\"](img)\n",
        "      output_c1 = net[\"clf1\"](feat)\n",
        "      output_c2 = net[\"clf2\"](feat)\n",
        "\n",
        "      # Apply the loss\n",
        "      test_loss += cost_function(output_c1, label).item()\n",
        "      output_ensemble = output_c1 + output_c2\n",
        "\n",
        "      # Predictions\n",
        "      predicted_1   = output_c1.argmax(dim=1)\n",
        "      predicted_2   = output_c2.argmax(dim=1)\n",
        "      pred_ensemble = output_ensemble.argmax(dim=1)\n",
        "\n",
        "      # Calculate accuracy\n",
        "      cumulative_accuracy[0] += predicted_1.eq(label).sum().item()\n",
        "      cumulative_accuracy[1] += predicted_2.eq(label).sum().item()\n",
        "      cumulative_accuracy[2] += pred_ensemble.eq(label).sum().item()\n",
        "\n",
        "      samples += img.shape[0]\n",
        "\n",
        "  test_loss = test_loss / samples\n",
        "  \n",
        "  free_GPU_memory(img, label)\n",
        "\n",
        "  return {\"test/test_loss\": test_loss, \n",
        "          \"test/test_accuracy1\": (cumulative_accuracy[0]/samples)*100,\n",
        "          \"test/test_accuracy2\": (cumulative_accuracy[1]/samples)*100,\n",
        "          \"test/test_accuracy_ensemble\": (cumulative_accuracy[2]/samples)*100}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vCYF6a6HWs5"
      },
      "source": [
        "### Declare the training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YU2RFdqjJCzA"
      },
      "outputs": [],
      "source": [
        "def training_MCD_DA(data_source, data_target, wandb_setup):\n",
        "    config = wandb_setup.config\n",
        "    print('CONFIGS\\n', yaml.dump(config._items, default_flow_style=False)) # Pretty print of configs\n",
        "\n",
        "    # Instantiates the network architecture \n",
        "    net = {\n",
        "      \"gen\" : Generator().to(DEVICE), \n",
        "      \"clf1\": Discriminator(NUM_CLASSES).to(DEVICE), \n",
        "      \"clf2\": Discriminator(NUM_CLASSES).to(DEVICE)\n",
        "      }\n",
        "\n",
        "    # Instantiates the optimizers\n",
        "    opt = {\n",
        "      \"gen\" : get_optimizer(net[\"gen\"].parameters() , config.optimizer, config.lr/10, config.wd), # small ResNet18 finetune \n",
        "      \"clf1\": get_optimizer(net[\"clf1\"].parameters(), config.optimizer, config.lr, config.wd), \n",
        "      \"clf2\": get_optimizer(net[\"clf2\"].parameters(), config.optimizer, config.lr, config.wd)\n",
        "      }\n",
        "\n",
        "    # Instantiates the schedulers\n",
        "    scheduler = {\n",
        "       \"clf1\": get_scheduler(opt['clf1'], GAMMA),\n",
        "       \"clf2\": get_scheduler(opt['clf2'], GAMMA)\n",
        "    }\n",
        "\n",
        "    # Instantiates the cost function\n",
        "    cost_function = get_cost_function().to(DEVICE)\n",
        "\n",
        "    best_acc = 0.\n",
        "    best_loss = 0.\n",
        "\n",
        "    # Loop epochs\n",
        "    for e in range(config.epochs):\n",
        "        print(f'-- Epoch [{e+1}/{config.epochs}] --')\n",
        "        train_metrics = training_step_MCD_DA(net, opt, scheduler, cost_function, data_source['train'], data_target['train'])\n",
        "        test_metrics = test_step_MCD_DA(net, cost_function, data_target['test'])\n",
        "        wandb.log({**train_metrics, **test_metrics})\n",
        "        print('Train: \\tLoss1: {:.6f}\\t Loss2: {:.6f}\\t Discrepancy: {:.6f}'.format(train_metrics[\"train/train_loss1\"], train_metrics[\"train/train_loss2\"], train_metrics[\"train/train_disc_loss\"]))\n",
        "        print('Test: \\tAverage loss: {:.6f}\\t Accuracy C1: {:.2f}%\\t Accuracy C2: {:.2f}%\\t Accuracy Ensemble: {:.2f}%'.format(test_metrics[\"test/test_loss\"], test_metrics[\"test/test_accuracy1\"], test_metrics[\"test/test_accuracy2\"], test_metrics[\"test/test_accuracy_ensemble\"]))\n",
        "    \n",
        "        if (best_acc < test_metrics[\"test/test_accuracy1\"]): # test_accuracy1 reported. Also the paper used it to display results\n",
        "            best_net = copy.deepcopy(net)\n",
        "            best_acc = test_metrics[\"test/test_accuracy1\"]\n",
        "            best_loss = test_metrics[\"test/test_loss\"]\n",
        "\n",
        "    os.makedirs(WEIGHTS_PATH + 'mcd_da/', exist_ok = True) \n",
        "    torch.save(best_net['gen'].state_dict(), WEIGHTS_PATH + 'mcd_da/' + config.name + '_G.pt')\n",
        "    torch.save(best_net['clf1'].state_dict(), WEIGHTS_PATH + 'mcd_da/' + config.name + '_C1.pt')\n",
        "    torch.save(best_net['clf2'].state_dict(), WEIGHTS_PATH + 'mcd_da/' + config.name + '_C2.pt')\n",
        "\n",
        "    visualize_results(best_net, data_source['test'], data_target['test'], ASSETS_PATH + 'mcd_da/' + config.name + '/')\n",
        "    \n",
        "    wandb.summary[\"test_best_acc\"] = best_acc\n",
        "    wandb.summary[\"test_best_loss\"] = best_loss\n",
        "    wandb.finish()\n",
        "\n",
        "    free_GPU_memory(net, best_net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyV_OuyTHWs6"
      },
      "source": [
        "### Let's train the MCD_DA!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrNRU0PbHWs6"
      },
      "source": [
        "**Train**: Product <br>\n",
        "**Test**: Real World \n",
        "\n",
        "Best test accuracy $acc = 85\\% $"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypNyRXaXYkPc"
      },
      "outputs": [],
      "source": [
        "NAME_RUN = \"MCD_DA_P_to_RW\"\n",
        "config={\n",
        "        \"backbone\": \"ResNet18\",\n",
        "        \"version\": \"DA\",\n",
        "        \"name\": NAME_RUN,\n",
        "        \"batch_size\": BATCH_SIZE,\n",
        "        \"epochs\": EPOCHS,\n",
        "        \"lr\": LR,\n",
        "        \"optimizer\": OPTIMIZER,\n",
        "        \"wd\": WD,\n",
        "        \"momentum\": MOMENTUM\n",
        "    }\n",
        "\n",
        "training_MCD_DA(product_data, rw_data, wandb.init(project=PROJECT_NAME, entity=ENTITY, name=NAME_RUN, mode=WANDB_MODE, config=config))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubzk7zcUhVsL"
      },
      "source": [
        "| t-SNE | Confusion matrix |\n",
        "|-|-|\n",
        "| ![tsne](https://drive.google.com/uc?export=view&id=1C7QauhRC335nGtimn1VXSE5e0yqT0aha) | ![cm](https://drive.google.com/uc?export=view&id=1yzh2zi5pLSBLLb-NmCDEZD8wAbITi-cd)|\n",
        "\n",
        "<br>\n",
        "\n",
        "|Classification report |\n",
        "|-|\n",
        "\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>precision</th>\n",
        "      <th>recall</th>\n",
        "      <th>f1-score</th>\n",
        "      <th>support</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>backpack</th>\n",
        "      <td>0.750000</td>\n",
        "      <td>0.882353</td>\n",
        "      <td>0.810811</td>\n",
        "      <td>17.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>bookcase</th>\n",
        "      <td>0.857143</td>\n",
        "      <td>0.600000</td>\n",
        "      <td>0.705882</td>\n",
        "      <td>20.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>car jack</th>\n",
        "      <td>0.750000</td>\n",
        "      <td>0.800000</td>\n",
        "      <td>0.774194</td>\n",
        "      <td>15.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>comb</th>\n",
        "      <td>0.857143</td>\n",
        "      <td>0.818182</td>\n",
        "      <td>0.837209</td>\n",
        "      <td>22.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>crown</th>\n",
        "      <td>1.000000</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>21.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>file cabinet</th>\n",
        "      <td>0.761905</td>\n",
        "      <td>0.727273</td>\n",
        "      <td>0.744186</td>\n",
        "      <td>22.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>flat iron</th>\n",
        "      <td>0.750000</td>\n",
        "      <td>0.937500</td>\n",
        "      <td>0.833333</td>\n",
        "      <td>16.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>game controller</th>\n",
        "      <td>1.000000</td>\n",
        "      <td>0.809524</td>\n",
        "      <td>0.894737</td>\n",
        "      <td>21.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>glasses</th>\n",
        "      <td>1.000000</td>\n",
        "      <td>0.894737</td>\n",
        "      <td>0.944444</td>\n",
        "      <td>19.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>helicopter</th>\n",
        "      <td>0.904762</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>0.950000</td>\n",
        "      <td>19.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>ice skates</th>\n",
        "      <td>0.894737</td>\n",
        "      <td>0.809524</td>\n",
        "      <td>0.850000</td>\n",
        "      <td>21.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>letter tray</th>\n",
        "      <td>0.718750</td>\n",
        "      <td>0.851852</td>\n",
        "      <td>0.779661</td>\n",
        "      <td>27.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>monitor</th>\n",
        "      <td>0.761905</td>\n",
        "      <td>0.800000</td>\n",
        "      <td>0.780488</td>\n",
        "      <td>20.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>mug</th>\n",
        "      <td>0.916667</td>\n",
        "      <td>0.916667</td>\n",
        "      <td>0.916667</td>\n",
        "      <td>24.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>network switch</th>\n",
        "      <td>0.923077</td>\n",
        "      <td>0.705882</td>\n",
        "      <td>0.800000</td>\n",
        "      <td>17.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>over-ear headphones</th>\n",
        "      <td>0.944444</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>0.971429</td>\n",
        "      <td>17.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>pen</th>\n",
        "      <td>0.764706</td>\n",
        "      <td>0.764706</td>\n",
        "      <td>0.764706</td>\n",
        "      <td>17.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>purse</th>\n",
        "      <td>0.894737</td>\n",
        "      <td>0.739130</td>\n",
        "      <td>0.809524</td>\n",
        "      <td>23.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>stand mixer</th>\n",
        "      <td>0.875000</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>0.933333</td>\n",
        "      <td>21.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>stroller</th>\n",
        "      <td>0.840000</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>0.913043</td>\n",
        "      <td>21.0000</td>\n",
        "    </tr>\n",
        "    <tr class=\"blank_row\">\n",
        "      <td colspan=\"6\"></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>accuracy</th>\n",
        "      <td>0.852500</td>\n",
        "      <td>0.852500</td>\n",
        "      <td>0.852500</td>\n",
        "      <td>0.8525</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>macro avg</th>\n",
        "      <td>0.858249</td>\n",
        "      <td>0.852866</td>\n",
        "      <td>0.850682</td>\n",
        "      <td>400.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>weighted avg</th>\n",
        "      <td>0.859320</td>\n",
        "      <td>0.852500</td>\n",
        "      <td>0.851100</td>\n",
        "      <td>400.0000</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "\n",
        "| Loss | Accuracy |\n",
        "|-|-|\n",
        "| ![loss](https://drive.google.com/uc?export=view&id=1O9ZxR0MH8CmfK989LoY5fcjn542ZnpMz) | ![accuracy](https://drive.google.com/uc?export=view&id=1TkLYfi79vBYV7VIydQlpH7N5QbpR0iJ7)|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Egq06Nh2mRKO"
      },
      "source": [
        "As expected, the accuracy obtained using an UDA framework is much higher with respect to the baseline.\n",
        "\n",
        "From the visualizations of the t-SNE and the confusion matrix, it is clear that the unsupervised domain adaptation framework provides superior results. Specifically, in the t-SNE chart, the clusters are more tightly grouped and well separated, indicating a higher level of class discrimination. Additionally, the confusion matrix shows that certain classes are classified completely correct, further demonstrating the effectiveness of the UDA framework.\n",
        "\n",
        "The chart of the accuracy results clearly illustrates that both discriminators achieve the same level of accuracy. This validates the authors' decision in the paper to adopt the first discriminator to compute the overall accuracy.\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSMUM01THWs6"
      },
      "source": [
        "**Train**: Real World <br>\n",
        "**Test**: Product \n",
        "\n",
        "Best test accuracy $acc = 94\\% $"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NQvjuPyHWs7"
      },
      "outputs": [],
      "source": [
        "NAME_RUN = \"MCD_DA_RW_to_P\"\n",
        "config={\n",
        "        \"backbone\": \"ResNet18\",\n",
        "        \"version\": \"DA\",\n",
        "        \"name\": NAME_RUN,\n",
        "        \"batch_size\": BATCH_SIZE,\n",
        "        \"epochs\": EPOCHS,\n",
        "        \"lr\": LR,\n",
        "        \"optimizer\": OPTIMIZER,\n",
        "        \"wd\": WD,\n",
        "        \"momentum\": MOMENTUM\n",
        "    }\n",
        "\n",
        "training_MCD_DA(rw_data, product_data, wandb.init(project=PROJECT_NAME, entity=ENTITY, name=NAME_RUN, mode=WANDB_MODE, config=config))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGRUMDXfizhF"
      },
      "source": [
        "| t-SNE | Confusion matrix |\n",
        "|-|-|\n",
        "| ![tsne](https://drive.google.com/uc?export=view&id=1IOM1Om8YWK1IYKDTwJuF2BKcW7pLJEcW) | ![cm](https://drive.google.com/uc?export=view&id=1x07d3VLT39QLA4h5E_ik8fMUrJV5Iu19)|\n",
        "\n",
        "<br>\n",
        "\n",
        "|Classification report |\n",
        "|-|\n",
        "\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>precision</th>\n",
        "      <th>recall</th>\n",
        "      <th>f1-score</th>\n",
        "      <th>support</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>backpack</th>\n",
        "      <td>0.965517</td>\n",
        "      <td>0.965517</td>\n",
        "      <td>0.965517</td>\n",
        "      <td>29.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>bookcase</th>\n",
        "      <td>0.869565</td>\n",
        "      <td>0.952381</td>\n",
        "      <td>0.909091</td>\n",
        "      <td>21.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>car jack</th>\n",
        "      <td>1.000000</td>\n",
        "      <td>0.882353</td>\n",
        "      <td>0.937500</td>\n",
        "      <td>17.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>comb</th>\n",
        "      <td>0.863636</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>0.926829</td>\n",
        "      <td>19.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>crown</th>\n",
        "      <td>1.000000</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>20.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>file cabinet</th>\n",
        "      <td>0.882353</td>\n",
        "      <td>0.833333</td>\n",
        "      <td>0.857143</td>\n",
        "      <td>18.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>flat iron</th>\n",
        "      <td>0.941176</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>0.969697</td>\n",
        "      <td>16.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>game controller</th>\n",
        "      <td>1.000000</td>\n",
        "      <td>0.916667</td>\n",
        "      <td>0.956522</td>\n",
        "      <td>24.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>glasses</th>\n",
        "      <td>1.000000</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>19.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>helicopter</th>\n",
        "      <td>0.894737</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>0.944444</td>\n",
        "      <td>17.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>ice skates</th>\n",
        "      <td>1.000000</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>19.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>letter tray</th>\n",
        "      <td>0.866667</td>\n",
        "      <td>0.812500</td>\n",
        "      <td>0.838710</td>\n",
        "      <td>16.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>monitor</th>\n",
        "      <td>1.000000</td>\n",
        "      <td>0.850000</td>\n",
        "      <td>0.918919</td>\n",
        "      <td>20.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>mug</th>\n",
        "      <td>0.944444</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>0.971429</td>\n",
        "      <td>17.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>network switch</th>\n",
        "      <td>0.958333</td>\n",
        "      <td>0.958333</td>\n",
        "      <td>0.958333</td>\n",
        "      <td>24.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>over-ear headphones</th>\n",
        "      <td>0.882353</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>0.937500</td>\n",
        "      <td>15.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>pen</th>\n",
        "      <td>0.923077</td>\n",
        "      <td>0.827586</td>\n",
        "      <td>0.872727</td>\n",
        "      <td>29.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>purse</th>\n",
        "      <td>1.000000</td>\n",
        "      <td>0.904762</td>\n",
        "      <td>0.950000</td>\n",
        "      <td>21.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>stand mixer</th>\n",
        "      <td>0.950000</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>0.974359</td>\n",
        "      <td>19.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>stroller</th>\n",
        "      <td>0.909091</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>0.952381</td>\n",
        "      <td>20.0000</td>\n",
        "    </tr>\n",
        "    <tr class=\"blank_row\">\n",
        "      <td colspan=\"6\"></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>accuracy</th>\n",
        "      <td>0.942500</td>\n",
        "      <td>0.942500</td>\n",
        "      <td>0.942500</td>\n",
        "      <td>0.9425</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>macro avg</th>\n",
        "      <td>0.942548</td>\n",
        "      <td>0.945172</td>\n",
        "      <td>0.942055</td>\n",
        "      <td>400.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>weighted avg</th>\n",
        "      <td>0.944951</td>\n",
        "      <td>0.942500</td>\n",
        "      <td>0.941970</td>\n",
        "      <td>400.0000</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "\n",
        "| Loss | Accuracy |\n",
        "|-|-|\n",
        "| ![loss](https://drive.google.com/uc?export=view&id=1v3V_eEVMkltbL4i0kQ8S6fBdRk8Atm-o) | ![accuracy](https://drive.google.com/uc?export=view&id=10_Sb_EsYz7xKn4ibztQLnGzSYGY6pZZq)|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXDYyTYqq7yh"
      },
      "source": [
        "Also for the $RW â P$ task, the UDA framework reaches an higher accuracy with respect to the baseline.\n",
        "\n",
        "As expected, as we have seen in the previous observation, in t-SNE chart the clusters are more tightly grouped and well separated. Additionally, the confusion matrix shows that we obtain better results, further demonstrating the effectiveness of the UDA framework.\n",
        "\n",
        "\n",
        "Moreover, also for the case $RW â P$ the two discriminators achieve the same level of accuracy.\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2XoQVB5HWs7"
      },
      "source": [
        "### Observations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mR-OFz3RHWs7"
      },
      "source": [
        "As expected, the method proposed by the paper [Maximum Classifier Discrepancy for Unsupervised Domain Adaptation](https://arxiv.org/abs/1712.02560) outperforms the baseline. From the results we obtained, we can demonstrate the effectiveness of the UDA device implemented.\n",
        "\n",
        "Firstly, we trained the networks using the SGD optimizer, but we have not reached big improvements with respect to the baseline. Then, we tried using the Adam optimizer and we performed some tests for the tuning of the hyperparameters. It is worth mentioning that for the feature extraction layer, i.e. Resnet18 without the classification layer, we decided to use a learning rate 10 times lower with respect to the learning rate used for other layers. \n",
        "\n",
        "It is interesting to visualize, using t-SNE, the features obtained by the last layer of the discriminator $F_1$ before applying a fully connected layer. Blue dots and red dots are, respectively, the test set source features and the test set target features. We report the $RW\\rightarrow P$ experiment.\n",
        "\n",
        "| Source only | Adapted |\n",
        "|-|-|\n",
        "| ![source-only](https://drive.google.com/uc?export=view&id=1-pOPCZTK-aYwIbN6HNyC2VtBWwcwTx5W) | ![adapted](https://drive.google.com/uc?export=view&id=1-HkjO2t1LmL5pdW923WJ1Ng_IbeucJN-)|\n",
        "\n",
        "We can appreciate the overlap of the features in the *Adapted* version, where we have a high accurate representation of the features.\n",
        "\n",
        "In the end, we obtained a gain of $9\\%$ in $P\\rightarrow RW$ and a gain of $2\\%$ in $RW\\rightarrow P$. Below, we report a table to summarize the results:\n",
        "\n",
        "|       | Baseline | MCD-DA | Gain |\n",
        "|-------|----------|--------|------|\n",
        "| $P\\rightarrow RW$ | $76\\%$      | $85\\%$    | $+9\\%$  |\n",
        "| $RW\\rightarrow P$ | $92\\%$      | $94\\%$  | $+2\\%$ |\n",
        "\n",
        "Overall, the paper presents a clear theoretical framework and we did not encounter any major challenges during the implementation.\n",
        "___\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRId_CFBshG4"
      },
      "source": [
        "## 3Â° implementation: Deep Subdomain Adaptation Network for image classification (DSAN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMbbH-pzHWs7"
      },
      "source": [
        "The paper [Deep Subdomain Adaptation Network](https://arxiv.org/abs/2106.09388) (DSAN), released in 2021 by Zhu et. al., presents a new method for adapting deep neural networks to new domains for image classification tasks. The proposed method utilizes a Local Maximum Mean Discrepancy (LMMD) loss to align the relevant subdomain distributions of domain-specific layer activations across different domains, rather than solely focusing on aligning the global source and target distributions as traditional methods do. The focus of this paper is on learning relevant subdomain adaptation through the LMMD loss, which results in improved classification accuracy on benchmark datasets.\n",
        "\n",
        "![](https://fuzhenzhuang.github.io/img/transfer/TNNLS2020_1.png)\n",
        "\n",
        "The loss of Subdomain Adaptation method is formulated as: $$ \\min_{f} \\frac{1}{n_s} \\sum_{i=1}^{n_s} \\mathcal{L_{ce}}(f(x_i^s), y_i^s) + \\lambda \\sum_{l\\in L} \\hat{d}_l(p,q)$$\n",
        "\n",
        "where $\\mathcal{L_{ce}}$ is the cross-entropy loss function and $\\hat{d}_l$ is the LMMD loss.\n",
        "\n",
        "The LMMD loss is formulated as: \n",
        "$$\n",
        "\\begin{aligned}\n",
        "& \\hat{d}_l(p, q)=\\frac{1}{C} \\sum_{c=1}^C\\left[\\sum_{i=1}^{n_s} \\sum_{j=1}^{n_s} w_i^{s c} w_j^{s c} k\\left(\\mathbf{z}_i^{s l}, \\mathbf{z}_j^{s l}\\right)\\right. \\left.+\\sum_{i=1}^{n_t} \\sum_{j=1}^{n_t} w_i^{t c} w_j^{t c} k\\left(\\mathbf{z}_i^{t l}, \\mathbf{z}_j^{t l}\\right)-2 \\sum_{i=1}^{n_s} \\sum_{j=1}^{n_t} w_i^{s c} w_j^{t c} k\\left(\\mathbf{z}_i^{s l}, \\mathbf{z}_j^{t l}\\right)\\right]\n",
        "\\end{aligned} $$\n",
        "\n",
        "Where $z^l$ is the l-th layer activation, $w_i^{sc}$ and $w_i^{tc}$ are the weight of $x_i^s$ and $x_i^t$ belonging to class $c$.\n",
        "\n",
        "$w_i^{c}$ is computed as: \n",
        "$w_i^{c}$ = $\\frac{y_{ic}}{\\sum_{(x_j, y_j)\\in D} y_{jc}}$\n",
        "\n",
        "Where $y_{ic}$ is the c-th entry of vector $y_i$. For samples in the source domain, the true label $y_i^s$ is used as a one-hot vector to compute $w_i^{sc}$ for each sample. However, in unsupervised adaptation we do not have labels, but only the output of the neural network $\\hat{y}_i = f(x_i)$. We can use $\\hat{y}_i^t$ as the probability of assigning $x_i^t$ to each of the C classes and then calculate $w_j^{tc}$ for each target sample.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFCweZehHWs8"
      },
      "source": [
        "### Local constants for DSAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7Im1DWxHWs8"
      },
      "outputs": [],
      "source": [
        "# training\n",
        "EPOCHS = 15\n",
        "OPTIMIZER = 'Adam' \n",
        "LR = 0.001\n",
        "WD = 5e-4\n",
        "MOMENTUM = 0.9\n",
        "GAMMA = 0.99\n",
        "GAMMA_DSAN = 10\n",
        "\n",
        "# LMMD loss\n",
        "KERNEL_NUM = 5 # number of kernel to compute for the bandwidth estimation \n",
        "KERNEL_MUL = 2. # multiplicative factor for the bandwidth estimation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qz8qydVNHWs8"
      },
      "source": [
        "### Loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxSPukZIHWs8"
      },
      "outputs": [],
      "source": [
        "# Code adapted from https://github.com/easezyc/deep-transfer-learning/blob/master/UDA/pytorch1.0/DSAN/lmmd.py\n",
        "\n",
        "class LMMDloss(torch.nn.Module):\n",
        "    def __init__(self, class_num, kernel_mul=KERNEL_MUL, kernel_num=KERNEL_NUM, fix_sigma=None):\n",
        "        super(LMMDloss, self).__init__()\n",
        "        self.class_num = class_num\n",
        "        self.kernel_num = kernel_num\n",
        "        self.kernel_mul = kernel_mul\n",
        "        self.fix_sigma = fix_sigma\n",
        "\n",
        "    def guassian_kernel(self, source, target, kernel_mul=KERNEL_MUL, kernel_num=KERNEL_NUM, fix_sigma=None):\n",
        "        n_samples = int(source.size()[0]) + int(target.size()[0])\n",
        "        total = torch.cat([source, target], dim=0)\n",
        "        total0 = total.unsqueeze(0).expand(\n",
        "            int(total.size(0)), int(total.size(0)), int(total.size(1)))\n",
        "        total1 = total.unsqueeze(1).expand(\n",
        "            int(total.size(0)), int(total.size(0)), int(total.size(1)))\n",
        "        L2_distance = ((total0-total1)**2).sum(2)\n",
        "        if fix_sigma:\n",
        "            bandwidth = fix_sigma\n",
        "        else:\n",
        "            bandwidth = torch.sum(L2_distance.data) / (n_samples**2-n_samples)\n",
        "        bandwidth /= kernel_mul ** (kernel_num // 2)\n",
        "        bandwidth_list = [bandwidth * (kernel_mul**i)\n",
        "                          for i in range(kernel_num)]\n",
        "        kernel_val = [torch.exp(-L2_distance / bandwidth_temp)\n",
        "                      for bandwidth_temp in bandwidth_list]\n",
        "        return sum(kernel_val)\n",
        "\n",
        "    def get_loss(self, source, target, s_label, t_label):\n",
        "        batch_size = source.size()[0]\n",
        "        weight_ss, weight_tt, weight_st = self.cal_weight(s_label, t_label, batch_size=batch_size, class_num=self.class_num)\n",
        "        weight_ss = torch.from_numpy(weight_ss).to(DEVICE)\n",
        "        weight_tt = torch.from_numpy(weight_tt).to(DEVICE)\n",
        "        weight_st = torch.from_numpy(weight_st).to(DEVICE)\n",
        "\n",
        "        kernels = self.guassian_kernel(source, target,kernel_mul=self.kernel_mul, kernel_num=self.kernel_num, fix_sigma=self.fix_sigma)\n",
        "        loss = torch.Tensor([0]).to(DEVICE)\n",
        "        if torch.sum(torch.isnan(sum(kernels))):\n",
        "            return loss\n",
        "        SS = kernels[:batch_size, :batch_size]\n",
        "        TT = kernels[batch_size:, batch_size:]\n",
        "        ST = kernels[:batch_size, batch_size:]\n",
        "\n",
        "        loss += torch.sum(weight_ss * SS + weight_tt * TT - 2 * weight_st * ST)\n",
        "        return loss\n",
        "\n",
        "    def convert_to_onehot(self, sca_label, class_num=31):\n",
        "        return np.eye(class_num)[sca_label]\n",
        "\n",
        "    def cal_weight(self, s_label, t_label, batch_size=32, class_num=31):\n",
        "        batch_size = s_label.size()[0]\n",
        "        s_sca_label = s_label.cpu().data.numpy()\n",
        "        s_vec_label = self.convert_to_onehot(s_sca_label, class_num=self.class_num)\n",
        "        s_sum = np.sum(s_vec_label, axis=0).reshape(1, class_num)\n",
        "        s_sum[s_sum == 0] = 100\n",
        "        s_vec_label = s_vec_label / s_sum\n",
        "\n",
        "        t_sca_label = t_label.cpu().data.max(1)[1].numpy()\n",
        "        t_vec_label = t_label.cpu().data.numpy()\n",
        "        t_sum = np.sum(t_vec_label, axis=0).reshape(1, class_num)\n",
        "        t_sum[t_sum == 0] = 100\n",
        "        t_vec_label = t_vec_label / t_sum\n",
        "\n",
        "        index = list(set(s_sca_label) & set(t_sca_label))\n",
        "        mask_arr = np.zeros((batch_size, class_num))\n",
        "        mask_arr[:, index] = 1\n",
        "        t_vec_label = t_vec_label * mask_arr\n",
        "        s_vec_label = s_vec_label * mask_arr\n",
        "\n",
        "        weight_ss = np.matmul(s_vec_label, s_vec_label.T)\n",
        "        weight_tt = np.matmul(t_vec_label, t_vec_label.T)\n",
        "        weight_st = np.matmul(s_vec_label, t_vec_label.T)\n",
        "\n",
        "        length = len(index)\n",
        "        if length != 0:\n",
        "            weight_ss = weight_ss / length\n",
        "            weight_tt = weight_tt / length\n",
        "            weight_st = weight_st / length\n",
        "        else:\n",
        "            weight_ss = np.array([0])\n",
        "            weight_tt = np.array([0])\n",
        "            weight_st = np.array([0])\n",
        "        return weight_ss.astype('float32'), weight_tt.astype('float32'), weight_st.astype('float32')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEu0D6DfHWs8"
      },
      "source": [
        "### Network architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C36BQKB4cAdL"
      },
      "source": [
        "The backbone model we use is the Resnet18. The network architecture is organized as follows:\n",
        "- Feature extractor: Resnet18 without the classification layer;\n",
        "- Fully connected layer;\n",
        "- Classification layer.\n",
        "\n",
        "Moreover, we implemented 3 forward methods for the model:\n",
        "- ``forward_DSAN``: this function is used for both the standard forward operation and the computation of the Local Maximum Mean Discrepancy (LMMD) loss. It takes as input source and target samples, as well as source labels, in order to calculate the LMMD loss which is used to align the relevant subdomain distributions of domain-specific layer activations across different domains;\n",
        "- ``forward_features``: it is used to obtain the features of the samples given as input. This function is useful for the plotting of the t-SNE;\n",
        "- ``forward``: this function is the standard forward operation of the network, in contrast to the forward_DSAN function which includes the computation of the Local Maximum Mean Discrepancy (LMMD) loss. It is used during the testing phase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzGjAFW_HWs8"
      },
      "outputs": [],
      "source": [
        "class DSAN_Network(torch.nn.Module):\n",
        "    def __init__(self, num_class):\n",
        "        super(DSAN_Network, self).__init__()\n",
        "        self.resnet = torchvision.models.resnet18(weights=WEIGHTS_RESNET18)\n",
        "        self.resnet_features = torch.nn.Sequential(*(list(self.resnet.children())[:-1]))\n",
        "        self.fc1 = torch.nn.Linear(512, 256)\n",
        "        self.cls = torch.nn.Linear(256, num_class)\n",
        "        self.lmmd_loss = LMMDloss(class_num = num_class)\n",
        "\n",
        "    def forward_DSAN(self, source, target, source_label):\n",
        "        source = self.resnet_features(source)\n",
        "        source = source.view(source.size(0), 512)\n",
        "        source = self.fc1(source)\n",
        "\n",
        "        target = self.resnet_features(target)\n",
        "        target = target.view(target.size(0), 512)\n",
        "        target = self.fc1(target)\n",
        "\n",
        "        source_pred = self.cls(source)\n",
        "        target_pred = self.cls(target)\n",
        "\n",
        "        lmmd = self.lmmd_loss.get_loss(source, target, source_label, F.softmax(target_pred, dim=1))\n",
        "        return source_pred, lmmd\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        x = self.resnet_features(x)\n",
        "        x = x.view(x.size(0), 512)\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.resnet_features(x)\n",
        "        x = x.view(x.size(0), 512)\n",
        "        x = self.fc1(x)\n",
        "        x = self.cls(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzSZKQDNHWs9"
      },
      "source": [
        "### Training step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYdSaMOxf7r3"
      },
      "source": [
        "The training phase, coherently with the Unsupervised Domain Adaptation approach, has been performed using only source samples, source labels and target samples. This means that the model is trained on the labeled source samples, and uses the target samples to align the feature representations and improve the model's ability to generalize to new domains.\n",
        "\n",
        "Moreover, the authors of the paper propose a progressive schedule for the adaptation factor $\\lambda$. Instead of fixing $\\lambda$ at a constant value, they gradually increase it from 0 to 1 to suppress noisy activations at the early stages of training.\n",
        "\n",
        "The schedule is defined by the following equation:\n",
        "$$\\lambda_Î¸ = \\frac {2}{exp(âÎ³Î¸)} â 1$$\n",
        "\n",
        "Following the paper, $Î³$ is a constant fixed equal to 10 throughout the experiments, and $Î¸$ is the training progress linearly changing from 0 to 1. This progressive schedule allows for a more effective and stable training process by gradually increasing the influence of the LMMD loss over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-cFc6ZhxHWs9"
      },
      "outputs": [],
      "source": [
        "def training_step_DSAN(model, optimizer, cost_function, source_train_loader, target_train_loader, total_epochs, current_epoch, scheduler):\n",
        "  source_samples = 0.\n",
        "  target_samples = 0.\n",
        "  cumulative_lmmd_loss = 0.\n",
        "  cumulative_total_loss = 0.\n",
        "  cumulative_ce_loss = 0.\n",
        "\n",
        "  target_iter = iter(target_train_loader)\n",
        "  model.train()\n",
        "\n",
        "  for batch_idx, (source, source_label) in enumerate(source_train_loader):\n",
        "      try:\n",
        "        target, _ = next(target_iter)\n",
        "      except:\n",
        "        target_iter = iter(target_train_loader)\n",
        "        target, _ = next(target_iter)\n",
        "      \n",
        "      source = source.to(DEVICE)\n",
        "      target = target.to(DEVICE)\n",
        "      source_label = source_label.to(DEVICE)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      source_samples += source.shape[0]\n",
        "      target_samples += target.shape[0]\n",
        "\n",
        "      # forward pass\n",
        "      source_output, lmmd_loss = model.forward_DSAN(source, target, source_label)\n",
        "\n",
        "      ce_loss = cost_function(source_output, source_label)\n",
        "\n",
        "      lambd = 2 / (1 + math.exp(-GAMMA_DSAN * (current_epoch) / total_epochs)) - 1\n",
        "\n",
        "      total_loss = ce_loss + (lambd * lmmd_loss)\n",
        "\n",
        "      total_loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      cumulative_lmmd_loss += lmmd_loss.item()\n",
        "      cumulative_total_loss += total_loss.item()\n",
        "      cumulative_ce_loss += ce_loss.item()\n",
        "  \n",
        "  scheduler.step()\n",
        "\n",
        "  free_GPU_memory(source, target, source_label)\n",
        "  \n",
        "  return {\"train/train_loss\": cumulative_total_loss/(source_samples + target_samples), \n",
        "          \"train/train_lmmd_loss\": cumulative_lmmd_loss/(source_samples + target_samples)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USTdHdmyHWs9"
      },
      "source": [
        "### Test step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3n2O9cWRjwX7"
      },
      "source": [
        "The testing phase, as stated in the introduction, has been performed only using target samples. This, in order to evaluate correctly the performance of the UDA device implemented for the project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XEhOS4uHWs9"
      },
      "outputs": [],
      "source": [
        "def test_step_DSAN(model, cost_function, target_test_loader):\n",
        "  samples = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "  \n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, (inputs, targets) in enumerate(target_test_loader):\n",
        "\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        targets = targets.to(DEVICE)\n",
        "\n",
        "        samples += inputs.shape[0]\n",
        "\n",
        "        output = model(inputs)\n",
        "        loss = cost_function(output, targets)\n",
        "\n",
        "        pred = output.argmax(dim=1)\n",
        "\n",
        "        cumulative_loss += loss.item()\n",
        "        cumulative_accuracy += pred.eq(targets).sum().item()\n",
        "\n",
        "  free_GPU_memory(inputs, targets)\n",
        "  \n",
        "  return {\"test/test_loss\": (cumulative_loss/samples), \n",
        "          \"test/test_acc\": (cumulative_accuracy/samples) * 100}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqOGCqYUHWs9"
      },
      "source": [
        "### Declare the training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGqCvFrLHWs-"
      },
      "outputs": [],
      "source": [
        "def training_DSAN(data_source, data_target, wandb_setup):\n",
        "  config = wandb_setup.config\n",
        "  print('CONFIGS\\n', yaml.dump(config._items, default_flow_style=False))\n",
        "\n",
        "  model = DSAN_Network(num_class = NUM_CLASSES).to(DEVICE)\n",
        "\n",
        "  optimizer = get_optimizer([{'params': model.resnet_features.parameters(), 'lr': config.lr/50},\n",
        "                             {'params': model.fc1.parameters()},\n",
        "                             {'params': model.cls.parameters()}], \n",
        "                             config.optimizer, config.lr, config.wd)\n",
        "\n",
        "  scheduler = get_scheduler(optimizer, GAMMA)\n",
        "  cost_function = get_cost_function()\n",
        "  \n",
        "  best_acc = 0.\n",
        "  best_loss = 0.\n",
        "\n",
        "  # Loop epochs\n",
        "  for e in range(config.epochs):\n",
        "      print(f'-- Epoch [{e+1}/{config.epochs}] --')\n",
        "      train_metrics = training_step_DSAN(model, optimizer, cost_function, data_source['train'], data_target['train'], config.epochs, e, scheduler)\n",
        "      test_metrics = test_step_DSAN(model, cost_function, data_target['test'])\n",
        "      wandb.log({**train_metrics, **test_metrics})\n",
        "      print('Train: \\tLoss: {:.6f}\\t LMMD loss: {:.6f}'.format(train_metrics[\"train/train_loss\"], train_metrics[\"train/train_lmmd_loss\"]))\n",
        "      print('Test: \\tAverage loss: {:.6f}\\t Accuracy: {:.2f}%'.format(test_metrics[\"test/test_loss\"], test_metrics[\"test/test_acc\"]))\n",
        "\n",
        "      if (best_acc < test_metrics[\"test/test_acc\"]):\n",
        "          best_model = copy.deepcopy(model)\n",
        "          best_acc = test_metrics[\"test/test_acc\"]\n",
        "          best_loss = test_metrics[\"test/test_loss\"]\n",
        "\n",
        "  os.makedirs(WEIGHTS_PATH + 'dsan/', exist_ok = True) \n",
        "  torch.save(best_model.state_dict(), WEIGHTS_PATH + 'dsan/' + config.name + '.pt')\n",
        "\n",
        "  visualize_results(best_model, data_source['test'], data_target['test'], ASSETS_PATH + 'dsan/' + config.name + '/')\n",
        "\n",
        "  wandb.summary[\"test_best_acc\"] = best_acc\n",
        "  wandb.summary[\"test_best_loss\"] = best_loss\n",
        "  wandb.finish()\n",
        "\n",
        "  free_GPU_memory(model, best_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HI9lUYQVHWs-"
      },
      "source": [
        "### Let's train the DSAN!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dR06A2mHWs-"
      },
      "source": [
        "**Train**: Product <br>\n",
        "**Test**: Real World \n",
        "\n",
        "Best test accuracy $acc = 87\\% $"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwWgM3AaHWs-"
      },
      "outputs": [],
      "source": [
        "NAME_RUN = \"DSAN_P_to_RW\"\n",
        "config={\n",
        "        \"backbone\": \"ResNet18\",\n",
        "        \"version\": \"DA\",\n",
        "        \"name\": NAME_RUN,\n",
        "        \"batch_size\": BATCH_SIZE,\n",
        "        \"epochs\": EPOCHS,\n",
        "        \"lr\": LR,\n",
        "        \"optimizer\": OPTIMIZER,\n",
        "        \"wd\": WD,\n",
        "        \"momentum\": MOMENTUM\n",
        "    }\n",
        "\n",
        "training_DSAN(product_data, rw_data, wandb.init(project=PROJECT_NAME, entity=ENTITY, name=NAME_RUN, mode=WANDB_MODE, config=config))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUckGTJ_rbF6"
      },
      "source": [
        "| t-SNE | Confusion matrix |\n",
        "|-|-|\n",
        "| ![tsne](https://drive.google.com/uc?export=view&id=1-cFR5VTD6YgKOj3tcOfo2s66ZZOcRfYt) | ![cm](https://drive.google.com/uc?export=view&id=1-aDZdGGWJenYg2DRWVo7sYYlakkayCgj)|\n",
        "\n",
        "<br>\n",
        "\n",
        "|Classification report |\n",
        "|-|\n",
        "\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>precision</th>\n",
        "      <th>recall</th>\n",
        "      <th>f1-score</th>\n",
        "      <th>support</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>backpack</th>\n",
        "      <td>0.739130</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>0.850000</td>\n",
        "      <td>17.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>bookcase</th>\n",
        "      <td>0.933333</td>\n",
        "      <td>0.700000</td>\n",
        "      <td>0.800000</td>\n",
        "      <td>20.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>car jack</th>\n",
        "      <td>0.933333</td>\n",
        "      <td>0.933333</td>\n",
        "      <td>0.933333</td>\n",
        "      <td>15.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>comb</th>\n",
        "      <td>0.888889</td>\n",
        "      <td>0.727273</td>\n",
        "      <td>0.800000</td>\n",
        "      <td>22.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>crown</th>\n",
        "      <td>0.954545</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>0.976744</td>\n",
        "      <td>21.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>file cabinet</th>\n",
        "      <td>0.642857</td>\n",
        "      <td>0.818182</td>\n",
        "      <td>0.720000</td>\n",
        "      <td>22.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>flat iron</th>\n",
        "      <td>0.882353</td>\n",
        "      <td>0.937500</td>\n",
        "      <td>0.909091</td>\n",
        "      <td>16.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>game controller</th>\n",
        "      <td>0.944444</td>\n",
        "      <td>0.809524</td>\n",
        "      <td>0.871795</td>\n",
        "      <td>21.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>glasses</th>\n",
        "      <td>1.000000</td>\n",
        "      <td>0.842105</td>\n",
        "      <td>0.914286</td>\n",
        "      <td>19.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>helicopter</th>\n",
        "      <td>1.000000</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>19.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>ice skates</th>\n",
        "      <td>0.900000</td>\n",
        "      <td>0.857143</td>\n",
        "      <td>0.878049</td>\n",
        "      <td>21.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>letter tray</th>\n",
        "      <td>0.793103</td>\n",
        "      <td>0.851852</td>\n",
        "      <td>0.821429</td>\n",
        "      <td>27.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>monitor</th>\n",
        "      <td>0.761905</td>\n",
        "      <td>0.800000</td>\n",
        "      <td>0.780488</td>\n",
        "      <td>20.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>mug</th>\n",
        "      <td>0.880000</td>\n",
        "      <td>0.916667</td>\n",
        "      <td>0.897959</td>\n",
        "      <td>24.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>network switch</th>\n",
        "      <td>0.750000</td>\n",
        "      <td>0.882353</td>\n",
        "      <td>0.810811</td>\n",
        "      <td>17.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>over-ear headphones</th>\n",
        "      <td>1.000000</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>17.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>pen</th>\n",
        "      <td>0.933333</td>\n",
        "      <td>0.823529</td>\n",
        "      <td>0.875000</td>\n",
        "      <td>17.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>purse</th>\n",
        "      <td>0.894737</td>\n",
        "      <td>0.739130</td>\n",
        "      <td>0.809524</td>\n",
        "      <td>23.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>stand mixer</th>\n",
        "      <td>0.840000</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>0.913043</td>\n",
        "      <td>21.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>stroller</th>\n",
        "      <td>0.944444</td>\n",
        "      <td>0.809524</td>\n",
        "      <td>0.871795</td>\n",
        "      <td>21.0000</td>\n",
        "    </tr>\n",
        "    <tr class=\"blank_row\">\n",
        "      <td colspan=\"6\"></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>accuracy</th>\n",
        "      <td>0.867500</td>\n",
        "      <td>0.867500</td>\n",
        "      <td>0.867500</td>\n",
        "      <td>0.8675</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>macro avg</th>\n",
        "      <td>0.880820</td>\n",
        "      <td>0.872406</td>\n",
        "      <td>0.871667</td>\n",
        "      <td>400.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>weighted avg</th>\n",
        "      <td>0.878169</td>\n",
        "      <td>0.867500</td>\n",
        "      <td>0.867910</td>\n",
        "      <td>400.0000</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "\n",
        "| Loss |\n",
        "|-|\n",
        "| ![loss](https://drive.google.com/uc?export=view&id=1J-n-YeVbYW8bU2PrCUz9ZdYjaxZK7Q56)|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DdPvIBYsS4p"
      },
      "source": [
        "The accuracy obtained with this UDA framework is higher with respect to the baseline and the MCD-DA method.\n",
        "\n",
        "The t-SNE and confusion matrix visualizations reveal that the unsupervised domain adaptation framework yields exceptional results. The t-SNE chart demonstrates this by showing clusters that are compact and well separated. The confusion matrix confirms this with the results obtained.\n",
        "\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-xJeyY_HWs-"
      },
      "source": [
        "**Train**: Real World <br>\n",
        "**Test**: Product \n",
        "\n",
        "Best test accuracy $acc = 94\\% $"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hMMe6SJHWs-"
      },
      "outputs": [],
      "source": [
        "NAME_RUN = \"DSAN_RW_to_P\"\n",
        "config={\n",
        "        \"backbone\": \"ResNet18\",\n",
        "        \"version\": \"DA\",\n",
        "        \"name\": NAME_RUN,\n",
        "        \"batch_size\": BATCH_SIZE,\n",
        "        \"epochs\": EPOCHS,\n",
        "        \"lr\": LR,\n",
        "        \"optimizer\": OPTIMIZER,\n",
        "        \"wd\": WD,\n",
        "        \"momentum\": MOMENTUM\n",
        "    }\n",
        "\n",
        "training_DSAN(rw_data, product_data, wandb.init(project=PROJECT_NAME, entity=ENTITY, name=NAME_RUN, mode=WANDB_MODE, config=config))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dgPgN-YsV4m"
      },
      "source": [
        "| t-SNE | Confusion matrix |\n",
        "|-|-|\n",
        "| ![tsne](https://drive.google.com/uc?export=view&id=1-jni6DdmqYE46zEdOl5-GlV1AkGloONI) | ![cm](https://drive.google.com/uc?export=view&id=1-jEj-xX3dyBZ34Z67bpY-09-qD0w_ne2)|\n",
        "\n",
        "<br>\n",
        "\n",
        "|Classification report |\n",
        "|-|\n",
        "\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>precision</th>\n",
        "      <th>recall</th>\n",
        "      <th>f1-score</th>\n",
        "      <th>support</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>backpack</th>\n",
        "      <td>0.933333</td>\n",
        "      <td>0.965517</td>\n",
        "      <td>0.949153</td>\n",
        "      <td>29.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>bookcase</th>\n",
        "      <td>0.833333</td>\n",
        "      <td>0.952381</td>\n",
        "      <td>0.888889</td>\n",
        "      <td>21.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>car jack</th>\n",
        "      <td>1.000000</td>\n",
        "      <td>0.882353</td>\n",
        "      <td>0.937500</td>\n",
        "      <td>17.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>comb</th>\n",
        "      <td>1.000000</td>\n",
        "      <td>0.947368</td>\n",
        "      <td>0.972973</td>\n",
        "      <td>19.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>crown</th>\n",
        "      <td>1.000000</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>20.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>file cabinet</th>\n",
        "      <td>0.875000</td>\n",
        "      <td>0.777778</td>\n",
        "      <td>0.823529</td>\n",
        "      <td>18.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>flat iron</th>\n",
        "      <td>0.941176</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>0.969697</td>\n",
        "      <td>16.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>game controller</th>\n",
        "      <td>1.000000</td>\n",
        "      <td>0.875000</td>\n",
        "      <td>0.933333</td>\n",
        "      <td>24.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>glasses</th>\n",
        "      <td>1.000000</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>19.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>helicopter</th>\n",
        "      <td>0.944444</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>0.971429</td>\n",
        "      <td>17.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>ice skates</th>\n",
        "      <td>1.000000</td>\n",
        "      <td>0.947368</td>\n",
        "      <td>0.972973</td>\n",
        "      <td>19.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>letter tray</th>\n",
        "      <td>1.000000</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>16.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>monitor</th>\n",
        "      <td>1.000000</td>\n",
        "      <td>0.950000</td>\n",
        "      <td>0.974359</td>\n",
        "      <td>20.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>mug</th>\n",
        "      <td>0.944444</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>0.971429</td>\n",
        "      <td>17.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>network switch</th>\n",
        "      <td>0.920000</td>\n",
        "      <td>0.958333</td>\n",
        "      <td>0.938776</td>\n",
        "      <td>24.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>over-ear headphones</th>\n",
        "      <td>0.789474</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>0.882353</td>\n",
        "      <td>15.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>pen</th>\n",
        "      <td>0.888889</td>\n",
        "      <td>0.827586</td>\n",
        "      <td>0.857143</td>\n",
        "      <td>29.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>purse</th>\n",
        "      <td>0.857143</td>\n",
        "      <td>0.857143</td>\n",
        "      <td>0.857143</td>\n",
        "      <td>21.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>stand mixer</th>\n",
        "      <td>1.000000</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>19.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>stroller</th>\n",
        "      <td>1.000000</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>1.000000</td>\n",
        "      <td>20.0000</td>\n",
        "    </tr>\n",
        "    <tr class=\"blank_row\">\n",
        "      <td colspan=\"6\"></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>accuracy</th>\n",
        "      <td>0.942500</td>\n",
        "      <td>0.942500</td>\n",
        "      <td>0.942500</td>\n",
        "      <td>0.9425</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>macro avg</th>\n",
        "      <td>0.946362</td>\n",
        "      <td>0.947041</td>\n",
        "      <td>0.945034</td>\n",
        "      <td>400.0000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>weighted avg</th>\n",
        "      <td>0.945466</td>\n",
        "      <td>0.942500</td>\n",
        "      <td>0.942450</td>\n",
        "      <td>400.0000</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "\n",
        "| Loss |\n",
        "|-|\n",
        "| ![loss](https://drive.google.com/uc?export=view&id=1DKvQX2QYsVB0fBThEwlGsIt4zkUWNhFE)|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Njl9JbIsoYM"
      },
      "source": [
        "Also for the task $RW â P$ the accuracy obtained with this UDA framework is higher with respect to the baseline.\n",
        "\n",
        "This is confirmed by the t-SNE and confusion matrix visualization. As stated before, also for this task the clusters of the t-SNE visualization are compact and well separated. As expected, also the confusion matrix confirms the superiority of this method.\n",
        "\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ryHJs3DHWs_"
      },
      "source": [
        "### Observations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gw90RSc08Xa-"
      },
      "source": [
        "The method proposed by the paper [Deep Subdomain Adaptation Network for image classification (DSAN)](https://arxiv.org/abs/2106.09388), outperforms the baseline too.\n",
        "\n",
        "We followed the same pipeline of the MCD-DA approach. We firstly started using SGD optimizer for the training of the network, but we have not reached big improvements with respect to the baseline. Then, with Adam optimizer with the right tuning of the hyperparameters we reached a huge improvement. As for the MCD-DA method, for the feature extraction network we decided to use a much smaller learning rate than the other parts of the model, specifically 50 times smaller. \n",
        "\n",
        "It is interesting to visualize, also in this method, the features obtained by the last layer of the fully connected layer before applying a linear classifier. Blue dots and red dots are, respectively, the test set source features and the test set target features. We report the $RW\\rightarrow P$ experiment.\n",
        "\n",
        "| Source only | Adapted |\n",
        "|-|-|\n",
        "| ![source-only](https://drive.google.com/uc?export=view&id=1-pOPCZTK-aYwIbN6HNyC2VtBWwcwTx5W) | ![adapted](https://drive.google.com/uc?export=view&id=1-g1308d0JtJBSeV2Sfm1hw8e2uzuu2M1)|\n",
        "\n",
        "We can appreciate the overlap of the features in the *Adapted* version, where we have a higher accurate representation of the features.\n",
        "\n",
        "The results we obtained with DSAN are very good, indeed we have a gain of $11%$ in $P\\rightarrow RW$ and a gain of $3\\%$ in $RW\\rightarrow P$. Below, we report a table to summarize the results:\n",
        "\n",
        "|       | Baseline | DSAN | Gain |\n",
        "|-------|----------|--------|------|\n",
        "| $P\\rightarrow RW$ | $76\\%$      | $87\\%$    | $+11\\%$  |\n",
        "| $RW\\rightarrow P$ | $92\\%$      | $94\\%$  | $+2\\%$ |\n",
        "\n",
        "Overall, the paper presents a clear theoretical framework and we did not encounter any major challenges during the implementation and even if it is a simple method, the results obtained are greater than the MCD-DA method and the baseline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIzVU3htrNhx"
      },
      "source": [
        "___\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRVBLSHiAVpU"
      },
      "source": [
        "## Final results consideration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHothmPpAcKq"
      },
      "source": [
        "We experienced several issues related to computational resources provided by the basic plan of Google Colab. The most annoying one was the GPU memory, in fact we had to restart the kernel several times to conclude all the tests for each method. We have not found any good solution online to fix the problem, but we tried to limitate it.\n",
        "\n",
        "In the following table we recap the gains obtained from each method, trained on Product $P$ and tested on Real World $RW$, and viceversa:\n",
        "\n",
        "\n",
        "|                    | MCD-DA | DSAN  |   \n",
        "|--------------------|--------|-------|\n",
        "| $$P \\rightarrow RW$$ Gain | $+9\\% $   | $+11\\%$ |   \n",
        "| $$RW \\rightarrow P$$ Gain | $+2\\%$   | $+2\\%$ |\n",
        "\n",
        "Even without utilizing the Unsupervised Domain Adaptation (UDA) framework, the baseline model demonstrates the ability to produce quite good results. This highlights the model's capability to effectively extract relevant features from the source domain, allowing accurate predictions in the target domain. \n",
        "\n",
        "Analyzing the upper bound results, it is demonstrated that learning from Product domain is easier than learning from the Real World one. This is due to the fact that for the network it is easier to extract meaningful features from the product domain.\n",
        "\n",
        "From the UDA perspective, it is clear that the gain we obtain from Product to Real World is much higher than Real World to Product. Furthermore, improving the $RW â P$ task is very challenging because the baseline accuracy is already 92%, which is close to the upper bound of Product domain, 96%.\n",
        "\n",
        "The paper [Maximum Classifier Discrepancy for Unsupervised Domain Adaptation](https://arxiv.org/abs/1712.02560), is a great improvement with respect to the baseline. We have an impressive improvement for both $P\\rightarrow RW$ and $RW\\rightarrow P$ tasks. It is explainable by the fact that the generator learns to generate discriminative features for target samples considering the relationship between the decision boundary and target samples, thanks to the discrepancy loss.\n",
        "\n",
        "Regarding the paper [Deep Subdomain Adaptation Network for image classification (DSAN)](https://arxiv.org/abs/2106.09388), it is evident that the UDA framework improves consistently the performance of the model for both $P\\rightarrow RW$ and $RW\\rightarrow P$ tasks, this means that it learns the alignment of the relevant subdomain distributions of domain-specific layer activations across different domains through the LMMD loss. Therefore, this method is much simpler with respect to MCD-DA, nevertheless it performs slightly better. Moreover, the training of this method is fairly faster because we observed that the MCD-DA method's steps are not optimized in terms of GPU computation.\n",
        "\n",
        "Thanks to both methods, we almost reach the upper-bound accuracy over the Product $P$ domain. What a time to be alive! *-Two Minutes Paper moment-* This achievement can be related to the quality of the data that we test, where there are no occlusions, the textures are well visible and there is no noise background."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tS2Sk5zoHWs_"
      },
      "source": [
        "## Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G23F3qg286jv"
      },
      "source": [
        "We trained, tested and analyzed three methods for Unsupervised Domain Adaptation. We started with a baseline using a [ResNet18](https://pytorch.org/hub/pytorch_vision_resnet/), then we moved to a method presented in 2018, [Maximum Classifier Discrepancy for Unsupervised Domain Adaptation](https://arxiv.org/abs/1712.02560), and finally to the last method presented in 2021, [Deep Subdomain Adaptation Network for Image Classification](https://arxiv.org/abs/2106.09388).\n",
        "\n",
        "We report again the table to summarize the results obtained with this project:\n",
        "\n",
        "|       | Baseline | MCD-DA | DSAN   |\n",
        "|-------|----------|--------|--------|\n",
        "| $P\\rightarrow RW$ | $76\\%$      | $85\\%$  | $\\mathbf{87\\%}$ |\n",
        "| $RW\\rightarrow P$ | $92\\%$      | $94\\%$  | $94\\%$ |\n",
        "\n",
        "We created a [GDrive folder](https://drive.google.com/drive/folders/1yyg4pHmEk3Jyc3T9xVX8M6z5nHpdpnhA?usp=sharing) to collect the entire project with the results's assets, model weights, link to the Adaptiope dataset and much more.\n",
        "\n",
        "We also share the interactive [WandB workspace](https://wandb.ai/dlfl/DL_UDA_2022), complete with all the models's runs.\n",
        "\n",
        "As future works, we would like to test it with different ResNet models to observe if a different feature generator improves the performance and, because we adopted only a subset of the Adaptiope dataset, we would like to see how the performances are affected by a large number of classes."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
